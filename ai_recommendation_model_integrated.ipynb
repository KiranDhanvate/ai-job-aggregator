{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPyHzaSLMfE1",
        "outputId": "f12e3b89-cdc1-4ac9-cc9c-939c2ce53497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üöÄ INSTALLING DEPENDENCIES\n",
            "================================================================================\n",
            "numpy\n",
            "pandas\n",
            "torch\n",
            "torchvision\n",
            "scikit-learn\n",
            "matplotlib\n",
            "seaborn\n",
            "tqdm\n",
            "fuzzywuzzy\n",
            "python-Levenshtein\n",
            "pdfplumber\n",
            "python-docx\n",
            "pillow\n",
            "requests\n",
            "beautifulsoup4\n",
            "python-jobspy\n",
            "ipywidgets\n",
            "\n",
            "Installation complete!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install packages with proper dependency handling\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üöÄ INSTALLING DEPENDENCIES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    packages = [\n",
        "        'numpy==2.0.2',\n",
        "        'pandas==2.1.4',\n",
        "        'torch',\n",
        "        'torchvision',\n",
        "        'scikit-learn',\n",
        "        'matplotlib',\n",
        "        'seaborn',\n",
        "        'tqdm',\n",
        "        'fuzzywuzzy',\n",
        "        'python-Levenshtein',\n",
        "        'pdfplumber',\n",
        "        'python-docx',\n",
        "        'pillow',\n",
        "        'requests',\n",
        "        'beautifulsoup4',\n",
        "        'python-jobspy',\n",
        "        'ipywidgets'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
        "            print(f\"{package.split('==')[0]}\")\n",
        "        except:\n",
        "            print(f\"{package.split('==')[0]} (may need runtime restart)\")\n",
        "\n",
        "    print(\"\\nInstallation complete!\\n\")\n",
        "\n",
        "# Install packages\n",
        "install_packages()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "# Real job scraping\n",
        "!pip install jobspy\n",
        "from jobspy import scrape_jobs\n",
        "\n",
        "\n",
        "# Resume parsing\n",
        "import pdfplumber\n",
        "import docx\n",
        "PDF_SUPPORT = True\n",
        "\n",
        "# File upload for Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    COLAB_ENV = True\n",
        "except:\n",
        "    COLAB_ENV = False\n",
        "    print(\"Not in Colab - file upload will use file paths\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üíª Device: {device}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxvTkG-KZBT2",
        "outputId": "84ac54d7-43e2-4594-ea71-7215a427e569"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jobspy in /usr/local/lib/python3.12/dist-packages (0.31.0)\n",
            "Requirement already satisfied: redis>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from jobspy) (7.0.1)\n",
            "‚úÖ All libraries imported successfully!\n",
            "üíª Device: cpu\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SKILLS DATABASE\n",
        "# ============================================================================\n",
        "\n",
        "SKILLS_DATABASE = [\n",
        "    # Programming Languages\n",
        "    'Python', 'Java', 'JavaScript', 'TypeScript', 'C++', 'C#', 'C', 'PHP',\n",
        "    'Ruby', 'Swift', 'Kotlin', 'Go', 'Rust', 'Scala', 'R', 'MATLAB',\n",
        "\n",
        "    # Web Frontend\n",
        "    'HTML', 'CSS', 'React', 'Angular', 'Vue.js', 'jQuery', 'Bootstrap',\n",
        "    'Tailwind CSS', 'SASS', 'LESS', 'Redux', 'Next.js',\n",
        "\n",
        "    # Web Backend\n",
        "    'Node.js', 'Express.js', 'Django', 'Flask', 'FastAPI', 'Spring Boot',\n",
        "    'ASP.NET', 'Ruby on Rails', 'Laravel', 'NestJS',\n",
        "\n",
        "    # Databases\n",
        "    'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQLite',\n",
        "    'Cassandra', 'DynamoDB', 'Elasticsearch', 'Neo4j', 'MariaDB',\n",
        "\n",
        "    # Data Science & ML\n",
        "    'TensorFlow', 'PyTorch', 'Keras', 'Scikit-learn', 'Pandas', 'NumPy',\n",
        "    'Matplotlib', 'Seaborn', 'OpenCV', 'NLTK', 'SpaCy', 'Transformers',\n",
        "    'Machine Learning', 'Deep Learning', 'Neural Networks', 'NLP',\n",
        "    'Computer Vision', 'Data Analysis',\n",
        "\n",
        "    # Big Data\n",
        "    'Spark', 'Hadoop', 'Kafka', 'Airflow', 'Databricks', 'Hive',\n",
        "\n",
        "    # Cloud Platforms\n",
        "    'AWS', 'Azure', 'GCP', 'Heroku', 'DigitalOcean', 'Firebase',\n",
        "\n",
        "    # DevOps & Tools\n",
        "    'Docker', 'Kubernetes', 'Jenkins', 'Git', 'GitHub', 'GitLab',\n",
        "    'CI/CD', 'Terraform', 'Ansible', 'Linux', 'Unix', 'Nginx',\n",
        "\n",
        "    # Mobile Development\n",
        "    'Android', 'iOS', 'React Native', 'Flutter', 'Xamarin', 'Ionic',\n",
        "\n",
        "    # Other Technologies\n",
        "    'REST API', 'GraphQL', 'Microservices', 'WebSocket', 'gRPC',\n",
        "    'Agile', 'Scrum', 'JIRA', 'Tableau', 'Power BI'\n",
        "]\n"
      ],
      "metadata": {
        "id": "jMB0J7gMZBbm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# RESUME PARSER\n",
        "# ============================================================================\n",
        "\n",
        "class ResumeParser:\n",
        "    \"\"\"Parse resume and extract skills\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.skills_db = SKILLS_DATABASE\n",
        "        self.email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "        self.phone_pattern = re.compile(r'[\\+]?[(]?[0-9]{1,4}[)]?[-\\s\\.]?[(]?[0-9]{1,4}[)]?[-\\s\\.]?[0-9]{1,9}')\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"Extract text from PDF\"\"\"\n",
        "        text = \"\"\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def extract_text_from_docx(self, docx_path):\n",
        "        \"\"\"Extract text from DOCX\"\"\"\n",
        "        doc = docx.Document(docx_path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "    def extract_skills(self, text):\n",
        "        \"\"\"Extract skills using word boundary matching\"\"\"\n",
        "        text_lower = ' ' + text.lower() + ' '\n",
        "        found_skills = set()\n",
        "\n",
        "        for skill in self.skills_db:\n",
        "            pattern = r'\\b' + re.escape(skill.lower()) + r'\\b'\n",
        "            if re.search(pattern, text_lower):\n",
        "                found_skills.add(skill)\n",
        "\n",
        "        return sorted(list(found_skills))\n",
        "\n",
        "    def extract_experience(self, text):\n",
        "        \"\"\"Extract years of experience\"\"\"\n",
        "        patterns = [\n",
        "            r'(\\d+)\\+?\\s*(?:years?|yrs?)(?:\\s+of)?\\s+(?:experience|exp)',\n",
        "            r'(?:experience|exp)(?:\\s+of)?\\s+(\\d+)\\+?\\s*(?:years?|yrs?)',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text.lower())\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "        return 0\n",
        "\n",
        "    def extract_education(self, text):\n",
        "        \"\"\"Extract education level\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        if any(kw in text_lower for kw in ['ph.d', 'phd', 'doctorate']):\n",
        "            return 'PhD'\n",
        "        elif any(kw in text_lower for kw in ['master', 'm.s', 'm.tech', 'mba', 'mca']):\n",
        "            return 'Masters'\n",
        "        elif any(kw in text_lower for kw in ['bachelor', 'b.e', 'b.tech', 'b.s', 'bca']):\n",
        "            return 'Bachelors'\n",
        "        return 'Bachelors'\n",
        "\n",
        "    def extract_contact(self, text):\n",
        "        \"\"\"Extract email and phone\"\"\"\n",
        "        emails = re.findall(self.email_pattern, text)\n",
        "        phones = re.findall(self.phone_pattern, text)\n",
        "\n",
        "        return {\n",
        "            'email': emails[0] if emails else 'not_found@example.com',\n",
        "            'phone': phones[0] if phones else 'Not provided'\n",
        "        }\n",
        "\n",
        "    def extract_name(self, text):\n",
        "        \"\"\"Extract candidate name\"\"\"\n",
        "        lines = [l.strip() for l in text.split('\\n') if l.strip()]\n",
        "        for line in lines[:5]:\n",
        "            if 2 <= len(line.split()) <= 4 and len(line) < 50:\n",
        "                return line\n",
        "        return \"Candidate\"\n",
        "\n",
        "    def parse_resume(self, file_path, file_type='pdf'):\n",
        "        \"\"\"Main parsing function\"\"\"\n",
        "        print(f\"\\nüìÑ Parsing resume: {os.path.basename(file_path)}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Extract text\n",
        "        if file_type.lower() == 'pdf':\n",
        "            text = self.extract_text_from_pdf(file_path)\n",
        "        elif file_type.lower() in ['docx', 'doc']:\n",
        "            text = self.extract_text_from_docx(file_path)\n",
        "        else:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                text = f.read()\n",
        "\n",
        "        if not text or len(text) < 50:\n",
        "            raise ValueError(\"Could not extract meaningful text from resume\")\n",
        "\n",
        "        # Extract all information\n",
        "        contact = self.extract_contact(text)\n",
        "        skills = self.extract_skills(text)\n",
        "        experience = self.extract_experience(text)\n",
        "        education = self.extract_education(text)\n",
        "        name = self.extract_name(text)\n",
        "\n",
        "        resume_data = {\n",
        "            'name': name,\n",
        "            'email': contact['email'],\n",
        "            'phone': contact['phone'],\n",
        "            'skills': skills,\n",
        "            'experience_years': experience,\n",
        "            'education_level': education,\n",
        "            'resume_text': text[:1000],\n",
        "            'full_text': text,\n",
        "            'parsed_date': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        print(f\"‚úÖ Successfully parsed resume!\\n\")\n",
        "        print(f\"Name: {name}\")\n",
        "        print(f\"Email: {contact['email']}\")\n",
        "        print(f\"Phone: {contact['phone']}\")\n",
        "        print(f\"Skills Found: {len(skills)}\")\n",
        "        print(f\"Experience: {experience} years\")\n",
        "        print(f\"Education: {education}\")\n",
        "\n",
        "        if skills:\n",
        "            print(f\"\\nüîß Top Skills: {', '.join(skills[:10])}\")\n",
        "            if len(skills) > 10:\n",
        "                print(f\"       ... and {len(skills) - 10} more\")\n",
        "\n",
        "        return resume_data\n"
      ],
      "metadata": {
        "id": "B7WBcjviZNfU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# REAL-TIME JOB SCRAPER\n",
        "# ============================================================================\n",
        "\n",
        "class RealTimeJobScraper:\n",
        "    \"\"\"Real-time job scraping using JobSpy\"\"\"\n",
        "\n",
        "    def scrape_jobs(self, location=\"India\", results_wanted=100, site_names=['indeed', 'linkedin'], is_remote=False):\n",
        "        \"\"\"Scrape jobs by location only\"\"\"\n",
        "        print(f\"\\nüîç SCRAPING REAL-TIME JOBS\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"   Location: {location}\")\n",
        "        print(f\"   Sites: {', '.join(site_names)}\")\n",
        "        print(f\"   Target: {results_wanted} jobs\")\n",
        "\n",
        "        try:\n",
        "            # Search with generic terms to get broad results\n",
        "            search_terms = [\"Software Developer\", \"Engineer\", \"Developer\"]\n",
        "            all_jobs = []\n",
        "\n",
        "            for term in search_terms:\n",
        "                jobs_df = scrape_jobs(\n",
        "                    site_name=site_names,\n",
        "                    search_term=term,\n",
        "                    location=location,\n",
        "                    results_wanted=results_wanted // len(search_terms),\n",
        "                    hours_old=168,  # Last week\n",
        "                    country_indeed='india',\n",
        "                    is_remote=is_remote,\n",
        "                    description_format='markdown'\n",
        "                )\n",
        "\n",
        "                if jobs_df is not None and len(jobs_df) > 0:\n",
        "                    all_jobs.append(jobs_df)\n",
        "\n",
        "            if not all_jobs:\n",
        "                print(\"\\n‚ùå No jobs found!\")\n",
        "                return []\n",
        "\n",
        "            # Combine all results\n",
        "            jobs_df = pd.concat(all_jobs, ignore_index=True)\n",
        "            jobs_df = jobs_df.drop_duplicates(subset=['title', 'company'], keep='first')\n",
        "\n",
        "            jobs = jobs_df.to_dict('records')\n",
        "\n",
        "            print(f\"\\n‚úÖ Successfully scraped {len(jobs)} REAL jobs!\")\n",
        "            print(f\"üìä Breakdown:\")\n",
        "\n",
        "            site_counts = jobs_df['site'].value_counts()\n",
        "            for site, count in site_counts.items():\n",
        "                print(f\"      - {site}: {count} jobs\")\n",
        "\n",
        "            return jobs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Scraping failed: {str(e)}\")\n",
        "            return []\n"
      ],
      "metadata": {
        "id": "Q4xkOsyFZNie"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# JOB PREPROCESSOR\n",
        "# ============================================================================\n",
        "\n",
        "class JobPreprocessor:\n",
        "    \"\"\"Process scraped jobs and extract features\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.skills_db = SKILLS_DATABASE\n",
        "\n",
        "    def extract_skills_from_description(self, description):\n",
        "        \"\"\"Extract skills from job description\"\"\"\n",
        "        if not description:\n",
        "            return []\n",
        "\n",
        "        text_lower = ' ' + str(description).lower() + ' '\n",
        "        found_skills = []\n",
        "\n",
        "        for skill in self.skills_db:\n",
        "            pattern = r'\\b' + re.escape(skill.lower()) + r'\\b'\n",
        "            if re.search(pattern, text_lower):\n",
        "                found_skills.append(skill)\n",
        "\n",
        "        return sorted(list(set(found_skills)))\n",
        "\n",
        "    def process_jobs(self, jobs_list):\n",
        "        \"\"\"Process and standardize job listings\"\"\"\n",
        "        print(f\"\\nüîß PROCESSING {len(jobs_list)} JOBS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        processed = []\n",
        "\n",
        "        for i, job in enumerate(jobs_list):\n",
        "            try:\n",
        "                desc = job.get('description', '')\n",
        "                if not desc:\n",
        "                    continue\n",
        "\n",
        "                skills = self.extract_skills_from_description(desc)\n",
        "\n",
        "                processed_job = {\n",
        "                    'job_id': f\"job_{i}_{job.get('site', 'unknown')}\",\n",
        "                    'title': job.get('title', 'Unknown Position'),\n",
        "                    'company': job.get('company', 'Unknown Company'),\n",
        "                    'location': job.get('location', 'Not specified'),\n",
        "                    'job_type': job.get('job_type', 'Full-time'),\n",
        "                    'description': str(desc)[:500],\n",
        "                    'requirements': str(desc)[:300],\n",
        "                    'skills': skills,\n",
        "                    'salary': self._format_salary(job),\n",
        "                    'date_posted': job.get('date_posted', 'Recently'),\n",
        "                    'job_url': job.get('job_url', '#'),\n",
        "                    'site': job.get('site', 'unknown')\n",
        "                }\n",
        "\n",
        "                processed.append(processed_job)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"‚úÖ Processed {len(processed)} jobs successfully\")\n",
        "\n",
        "        if processed:\n",
        "            skill_counts = [len(j['skills']) for j in processed]\n",
        "            print(f\"üìä Skills per job: avg {np.mean(skill_counts):.1f}, max {max(skill_counts)}\")\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def _format_salary(self, job):\n",
        "        \"\"\"Format salary information\"\"\"\n",
        "        if job.get('min_amount') and job.get('max_amount'):\n",
        "            interval = job.get('interval', 'yearly')\n",
        "            return f\"${job['min_amount']:,} - ${job['max_amount']:,} ({interval})\"\n",
        "        return \"Not specified\""
      ],
      "metadata": {
        "id": "Z7UmpTT6ZBf8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ConvFM MODEL COMPONENTS (From Untitled34.ipynb)\n",
        "# ============================================================================\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    \"\"\"CNN for text feature extraction\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim=128, num_filters=64, filter_sizes=[3, 4, 5], dropout=0.5):\n",
        "        super(TextCNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_dim = num_filters * len(filter_sizes)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text).unsqueeze(1)\n",
        "\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            conv_out = F.relu(conv(embedded))\n",
        "            pooled = F.max_pool2d(conv_out, (conv_out.size(2), 1))\n",
        "            conv_outputs.append(pooled.squeeze(3).squeeze(2))\n",
        "\n",
        "        features = torch.cat(conv_outputs, dim=1)\n",
        "        return self.dropout(features)\n",
        "\n",
        "class SkillsEncoder(nn.Module):\n",
        "    \"\"\"Encode skills into dense representation\"\"\"\n",
        "\n",
        "    def __init__(self, num_skills, embedding_dim=32, dropout=0.3):\n",
        "        super(SkillsEncoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(num_skills, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, embedding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, skill_vector):\n",
        "        return self.encoder(skill_vector)\n",
        "\n",
        "class FactorizationMachine(nn.Module):\n",
        "    \"\"\"Factorization Machine for modeling interactions\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, factor_dim=32):\n",
        "        super(FactorizationMachine, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
        "        self.factor_embeddings = nn.Parameter(torch.randn(input_dim, factor_dim) * 0.01)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.factor_embeddings)\n",
        "        nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        linear_term = self.linear(x)\n",
        "\n",
        "        x_expanded = x.unsqueeze(2)\n",
        "        factor_mul = x_expanded * self.factor_embeddings.unsqueeze(0)\n",
        "\n",
        "        sum_square = torch.sum(factor_mul, dim=1) ** 2\n",
        "        square_sum = torch.sum(factor_mul ** 2, dim=1)\n",
        "\n",
        "        interaction_term = 0.5 * torch.sum(sum_square - square_sum, dim=1, keepdim=True)\n",
        "\n",
        "        output = linear_term + interaction_term\n",
        "        return output\n",
        "\n",
        "class ConvFM(nn.Module):\n",
        "    \"\"\"Complete ConvFM Model\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, num_skills, embedding_dim=128, cnn_num_filters=64,\n",
        "                 cnn_filter_sizes=[3, 4, 5], fm_factor_dim=32, skills_embed_dim=32, dropout=0.5):\n",
        "        super(ConvFM, self).__init__()\n",
        "\n",
        "        self.text_cnn = TextCNN(vocab_size, embedding_dim, cnn_num_filters, cnn_filter_sizes, dropout)\n",
        "        self.skills_encoder = SkillsEncoder(num_skills, skills_embed_dim, dropout * 0.6)\n",
        "\n",
        "        self.cat_embeddings = nn.ModuleDict({\n",
        "            'job_type': nn.Embedding(20, 16),\n",
        "            'location': nn.Embedding(500, 32),\n",
        "            'education_level': nn.Embedding(10, 8)\n",
        "        })\n",
        "\n",
        "        self.cnn_dim = self.text_cnn.output_dim\n",
        "        self.skills_dim = skills_embed_dim\n",
        "        self.cat_dim = sum(emb.embedding_dim for emb in self.cat_embeddings.values()) # Calculate cat_dim dynamically\n",
        "        self.total_dim = (self.cnn_dim + self.skills_dim + self.cat_dim) * 2\n",
        "\n",
        "        self.fm = FactorizationMachine(self.total_dim, fm_factor_dim)\n",
        "        self.batch_norm = nn.BatchNorm1d(self.total_dim)\n",
        "\n",
        "    def extract_features(self, text, skills, categorical):\n",
        "        text_features = self.text_cnn(text)\n",
        "        skills_features = self.skills_encoder(skills)\n",
        "\n",
        "        cat_features = []\n",
        "        # Ensure all categorical embeddings are processed, even if input is None\n",
        "        for name, emb_layer in self.cat_embeddings.items():\n",
        "             if name in categorical and categorical[name] is not None:\n",
        "                cat_features.append(emb_layer(categorical[name]))\n",
        "             else:\n",
        "                 # Create a zero tensor with the correct embedding dimension if category is missing\n",
        "                 cat_features.append(torch.zeros(text.size(0), emb_layer.embedding_dim, device=text.device))\n",
        "\n",
        "        cat_features = torch.cat(cat_features, dim=1)\n",
        "        combined = torch.cat([text_features, skills_features, cat_features], dim=1)\n",
        "        return combined\n",
        "\n",
        "\n",
        "    def forward(self, user_text, job_text, user_skills, job_skills, user_categorical, job_categorical):\n",
        "        user_features = self.extract_features(user_text, user_skills, user_categorical)\n",
        "        job_features = self.extract_features(job_text, job_skills, job_categorical)\n",
        "\n",
        "        combined_features = torch.cat([user_features, job_features], dim=1)\n",
        "        combined_features = self.batch_norm(combined_features)\n",
        "\n",
        "        scores = self.fm(combined_features)\n",
        "        scores = torch.sigmoid(scores)\n",
        "\n",
        "        return scores"
      ],
      "metadata": {
        "id": "Qm5XcBM7ZTXs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VOCABULARY BUILDER\n",
        "# ============================================================================\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Build and manage vocabulary\"\"\"\n",
        "\n",
        "    def __init__(self, max_vocab_size=5000):\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
        "        self.word_freq = Counter()\n",
        "\n",
        "    def build_from_texts(self, texts):\n",
        "        for text in texts:\n",
        "            tokens = self._tokenize(text)\n",
        "            self.word_freq.update(tokens)\n",
        "\n",
        "        most_common = self.word_freq.most_common(self.max_vocab_size - 2)\n",
        "\n",
        "        for idx, (word, freq) in enumerate(most_common, start=2):\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-z0-9\\s\\+\\#\\.]', ' ', text)\n",
        "        return text.split()\n",
        "\n",
        "    def text_to_sequence(self, text, max_length=200):\n",
        "        tokens = self._tokenize(text)\n",
        "        indices = [self.word2idx.get(token, 1) for token in tokens]\n",
        "\n",
        "        if len(indices) < max_length:\n",
        "            indices += [0] * (max_length - len(indices))\n",
        "        else:\n",
        "            indices = indices[:max_length]\n",
        "\n",
        "        return indices"
      ],
      "metadata": {
        "id": "ZpykdVvYZU-1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ConvFM-BASED JOB RECOMMENDER\n",
        "# ============================================================================\n",
        "\n",
        "class ConvFMJobRecommender:\n",
        "    \"\"\"Job recommender using trained ConvFM model\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.parser = ResumeParser()\n",
        "        self.scraper = RealTimeJobScraper()\n",
        "        self.processor = JobPreprocessor()\n",
        "        self.model = None\n",
        "        self.vocabulary = None\n",
        "        self.label_encoders = {}\n",
        "        self.device = device\n",
        "\n",
        "    def initialize_model(self, user_profile, jobs, location):\n",
        "        \"\"\"Initialize and train ConvFM model\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üß† INITIALIZING ConvFM MODEL\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Build vocabulary\n",
        "        print(\"\\nüìö Building vocabulary...\")\n",
        "        self.vocabulary = Vocabulary(max_vocab_size=5000)\n",
        "        all_texts = [user_profile['resume_text']]\n",
        "        all_texts.extend([j['description'] + ' ' + j['requirements'] for j in jobs])\n",
        "        self.vocabulary.build_from_texts(all_texts)\n",
        "        print(f\"‚úÖ Vocabulary size: {len(self.vocabulary.word2idx)}\")\n",
        "\n",
        "        # Initialize label encoders\n",
        "        self.label_encoders['job_type'] = LabelEncoder()\n",
        "        self.label_encoders['location'] = LabelEncoder()\n",
        "        self.label_encoders['education'] = LabelEncoder()\n",
        "\n",
        "        all_job_types = [j.get('job_type', 'Full-time') for j in jobs] + ['Full-time']\n",
        "        all_locations = [j.get('location', 'Remote') for j in jobs] + ['Remote', location] # Include specified location\n",
        "        all_education = ['Bachelors', 'Masters', 'PhD']\n",
        "\n",
        "        self.label_encoders['job_type'].fit(all_job_types)\n",
        "        self.label_encoders['location'].fit(all_locations)\n",
        "        self.label_encoders['education'].fit(all_education)\n",
        "\n",
        "        # Initialize ConvFM model\n",
        "        print(\"\\nüèóÔ∏è Building ConvFM model...\")\n",
        "        self.model = ConvFM(\n",
        "            vocab_size=len(self.vocabulary.word2idx),\n",
        "            num_skills=len(SKILLS_DATABASE),\n",
        "            embedding_dim=128,\n",
        "            cnn_num_filters=64,\n",
        "            cnn_filter_sizes=[3, 4, 5],\n",
        "            fm_factor_dim=32,\n",
        "            skills_embed_dim=32,\n",
        "            dropout=0.5\n",
        "        ).to(self.device)\n",
        "\n",
        "        print(f\"‚úÖ Model initialized with {sum(p.numel() for p in self.model.parameters()):,} parameters\")\n",
        "\n",
        "        # Train model with synthetic interactions\n",
        "        self._train_model_quick(user_profile, jobs)\n",
        "\n",
        "    def _train_model_quick(self, user_profile, jobs):\n",
        "        \"\"\"Quick training with synthetic data\"\"\"\n",
        "        print(\"\\nüéØ Quick training ConvFM...\")\n",
        "\n",
        "        # Generate synthetic training data\n",
        "        user_skills_set = set(user_profile['skills'])\n",
        "        train_data = []\n",
        "\n",
        "        for job in jobs[:min(len(jobs), 50)]:  # Use subset for quick training\n",
        "            job_skills_set = set(job['skills'])\n",
        "            skill_match = len(user_skills_set & job_skills_set) / max(len(job_skills_set), 1)\n",
        "\n",
        "            # Add more noise and scale the label to provide more variation\n",
        "            label = min(1.0, max(0.0, skill_match * 0.7 + np.random.normal(0, 0.3))) # Increased noise and scaling\n",
        "            train_data.append((job, label))\n",
        "\n",
        "        if not train_data:\n",
        "            print(\"‚ö†Ô∏è No training data generated, skipping training\")\n",
        "            return\n",
        "\n",
        "        # Create DataLoader for batching\n",
        "        class TrainingDataset(Dataset):\n",
        "            def __init__(self, user_profile, jobs, vocabulary, label_encoders):\n",
        "                self.user_profile = user_profile\n",
        "                self.jobs = jobs\n",
        "                self.vocabulary = vocabulary\n",
        "                self.label_encoders = label_encoders\n",
        "                self.user_skills_vec = self._get_skills_vector(user_profile['skills'])\n",
        "                self.user_cat = {\n",
        "                    'job_type': label_encoders['job_type'].transform(['Full-time'])[0],\n",
        "                    'location': label_encoders['location'].transform([user_profile.get('location', 'Remote')])[0], # Use user profile location or default\n",
        "                    'education_level': label_encoders['education'].transform([user_profile['education_level']])[0]\n",
        "                }\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.jobs)\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                job = self.jobs[idx]\n",
        "                job_skills_set = set(job['skills'])\n",
        "                skill_match = len(set(self.user_profile['skills']) & job_skills_set) / max(len(job_skills_set), 1)\n",
        "                label = min(1.0, max(0.0, skill_match * 0.7 + np.random.normal(0, 0.3))) # Consistent label generation\n",
        "\n",
        "                user_text = self.vocabulary.text_to_sequence(self.user_profile['resume_text'])\n",
        "                job_text = self.vocabulary.text_to_sequence(job['description'] + ' ' + job['requirements'])\n",
        "\n",
        "                job_skills_vec = self._get_skills_vector(job['skills'])\n",
        "\n",
        "                job_cat = {\n",
        "                    'job_type': self.label_encoders['job_type'].transform([job.get('job_type', 'Full-time')])[0],\n",
        "                    'location': self.label_encoders['location'].transform([job.get('location', 'Remote')])[0],\n",
        "                    'education_level': 0 # Default for job education\n",
        "                }\n",
        "\n",
        "                return (torch.LongTensor(user_text), torch.LongTensor(job_text),\n",
        "                        torch.FloatTensor(self.user_skills_vec), torch.FloatTensor(job_skills_vec),\n",
        "                        self.user_cat, job_cat, torch.FloatTensor([label]))\n",
        "\n",
        "            def _get_skills_vector(self, skills):\n",
        "                vector = np.zeros(len(SKILLS_DATABASE))\n",
        "                for skill in skills:\n",
        "                    if skill in SKILLS_DATABASE:\n",
        "                        idx = SKILLS_DATABASE.index(skill)\n",
        "                        vector[idx] = 1\n",
        "                return vector\n",
        "\n",
        "\n",
        "        train_dataset = TrainingDataset(user_profile, [job for job, _ in train_data], self.vocabulary, self.label_encoders)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True) # Use batch_size > 1\n",
        "\n",
        "        # Training loop\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        self.model.train()\n",
        "        num_epochs = 15 # Increased epochs\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            total_loss = 0\n",
        "            for user_text, job_text, user_skills_vec, job_skills_vec, user_cat, job_cat, labels in train_dataloader:\n",
        "\n",
        "                user_text = user_text.to(self.device)\n",
        "                job_text = job_text.to(self.device)\n",
        "                user_skills_vec = user_skills_vec.to(self.device)\n",
        "                job_skills_vec = job_skills_vec.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                user_cat_processed = {k: v.to(self.device) for k, v in user_cat.items()}\n",
        "                job_cat_processed = {k: v.to(self.device) for k, v in job_cat.items()}\n",
        "\n",
        "\n",
        "                # Forward pass\n",
        "                pred = self.model(user_text, job_text, user_skills_vec, job_skills_vec, user_cat_processed, job_cat_processed)\n",
        "                loss = criterion(pred, labels)\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(train_dataloader)\n",
        "            print(f\"   Epoch {epoch + 1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        print(\"‚úÖ Model training complete!\")\n",
        "\n",
        "    def _get_skills_vector(self, skills):\n",
        "        \"\"\"Convert skills to one-hot vector\"\"\"\n",
        "        vector = np.zeros(len(SKILLS_DATABASE))\n",
        "        for skill in skills:\n",
        "            if skill in SKILLS_DATABASE:\n",
        "                idx = SKILLS_DATABASE.index(skill)\n",
        "                vector[idx] = 1\n",
        "        return vector\n",
        "\n",
        "    def recommend_from_resume(self, resume_path, resume_type='pdf', location=\"India\",\n",
        "                            num_jobs=100, top_k=15, min_skill_match=0.3, site_names=['indeed', 'linkedin']):\n",
        "        \"\"\"Complete recommendation pipeline using ConvFM\"\"\"\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üöÄ STARTING ConvFM JOB RECOMMENDATION\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # STEP 1: Parse Resume\n",
        "        user_profile = self.parser.parse_resume(resume_path, resume_type)\n",
        "\n",
        "        if not user_profile['skills']:\n",
        "            print(\"\\n‚ùå ERROR: No skills found in resume!\")\n",
        "            print(\"Please ensure your resume contains technical skills.\")\n",
        "            return None\n",
        "\n",
        "        # STEP 2: Scrape Real Jobs\n",
        "        raw_jobs = self.scraper.scrape_jobs(\n",
        "            location=location,\n",
        "            results_wanted=num_jobs,\n",
        "            site_names=site_names,\n",
        "            is_remote=False\n",
        "        )\n",
        "\n",
        "        if not raw_jobs:\n",
        "            print(\"\\n‚ùå ERROR: No jobs found!\")\n",
        "            return None\n",
        "\n",
        "        # STEP 3: Process Jobs\n",
        "        jobs = self.processor.process_jobs(raw_jobs)\n",
        "\n",
        "        if not jobs:\n",
        "            print(\"\\n‚ùå ERROR: No valid jobs after processing!\")\n",
        "            return None\n",
        "\n",
        "        # STEP 4: Initialize and Train ConvFM Model\n",
        "        self.initialize_model(user_profile, jobs, location) # Pass location to initialize_model\n",
        "\n",
        "        # STEP 5: Get ConvFM Predictions and Match Jobs\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üéØ GENERATING RECOMMENDATIONS WITH ConvFM\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        recommendations = []\n",
        "        user_skills_set = set(user_profile['skills'])\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Prepare data for batch processing\n",
        "            job_texts = []\n",
        "            job_skills_vectors = []\n",
        "            job_categorical_data = {k: [] for k in self.label_encoders.keys()}\n",
        "            job_list_filtered = [] # Store jobs that pass skill match threshold\n",
        "\n",
        "            for job in jobs:\n",
        "                job_skills_set = set(job['skills'])\n",
        "                if not job_skills_set:\n",
        "                    skill_match = 0.0\n",
        "                else:\n",
        "                    skill_match = len(user_skills_set & job_skills_set) / len(job_skills_set)\n",
        "\n",
        "                if skill_match < min_skill_match:\n",
        "                    continue\n",
        "\n",
        "                job_texts.append(self.vocabulary.text_to_sequence(job['description'] + ' ' + job['requirements']))\n",
        "                job_skills_vectors.append(self._get_skills_vector(job['skills']))\n",
        "                job_categorical_data['job_type'].append(self.label_encoders['job_type'].transform([job.get('job_type', 'Full-time')])[0])\n",
        "                job_categorical_data['location'].append(self.label_encoders['location'].transform([job.get('location', 'Remote')])[0])\n",
        "                job_categorical_data['education'].append(0) # Default for job education\n",
        "                job_list_filtered.append(job)\n",
        "\n",
        "\n",
        "            if not job_list_filtered:\n",
        "                print(f\"\\n‚ùå No jobs found with at least {min_skill_match*100:.0f}% skill match!\")\n",
        "                print(\"Try lowering the minimum skill match threshold.\")\n",
        "                return None\n",
        "\n",
        "            # Convert lists to tensors\n",
        "            job_texts_tensor = torch.LongTensor(job_texts).to(self.device)\n",
        "            job_skills_vectors_tensor = torch.FloatTensor(job_skills_vectors).to(self.device)\n",
        "            job_categorical_tensors = {k: torch.LongTensor(v).to(self.device) for k, v in job_categorical_data.items()}\n",
        "\n",
        "            user_text_tensor = torch.LongTensor(\n",
        "                self.vocabulary.text_to_sequence(user_profile['resume_text'])\n",
        "            ).unsqueeze(0).repeat(len(job_list_filtered), 1).to(self.device) # Repeat for batch\n",
        "\n",
        "            user_skills_vec_tensor = torch.FloatTensor(\n",
        "                self._get_skills_vector(user_profile['skills'])\n",
        "            ).unsqueeze(0).repeat(len(job_list_filtered), 1).to(self.device) # Repeat for batch\n",
        "\n",
        "            user_cat_tensor = {\n",
        "                'job_type': torch.LongTensor([self.label_encoders['job_type'].transform(['Full-time'])[0]]).repeat(len(job_list_filtered)).to(self.device),\n",
        "                'location': torch.LongTensor([self.label_encoders['location'].transform([location])[0]]).repeat(len(job_list_filtered)).to(self.device),\n",
        "                'education_level': torch.LongTensor([self.label_encoders['education'].transform([user_profile['education_level']])[0]]).repeat(len(job_list_filtered)).to(self.device)\n",
        "            }\n",
        "\n",
        "\n",
        "            # Process in batches\n",
        "            batch_size = 16 # Choose a suitable batch size\n",
        "            all_model_scores = []\n",
        "\n",
        "            for i in tqdm(range(0, len(job_list_filtered), batch_size), desc=\"Scoring jobs\"):\n",
        "                batch_job_texts = job_texts_tensor[i:i+batch_size]\n",
        "                batch_job_skills_vectors = job_skills_vectors_tensor[i:i+batch_size]\n",
        "                batch_job_categorical = {k: v[i:i+batch_size] for k, v in job_categorical_tensors.items()}\n",
        "\n",
        "                batch_user_text = user_text_tensor[i:i+batch_size]\n",
        "                batch_user_skills_vec = user_skills_vec_tensor[i:i+batch_size]\n",
        "                batch_user_cat = {k: v[i:i+batch_size] for k, v in user_cat_tensor.items()}\n",
        "\n",
        "\n",
        "                model_scores = self.model(\n",
        "                    batch_user_text, batch_job_texts,\n",
        "                    batch_user_skills_vec, batch_job_skills_vectors,\n",
        "                    batch_user_cat, batch_job_categorical\n",
        "                ).squeeze(1).tolist()\n",
        "                all_model_scores.extend(model_scores)\n",
        "\n",
        "            # Combine scores and create recommendations\n",
        "            for i, job in enumerate(job_list_filtered):\n",
        "                job_skills_set = set(job['skills'])\n",
        "                user_skills_set = set(user_profile['skills'])\n",
        "                matching_skills = list(user_skills_set & job_skills_set)\n",
        "                missing_skills = list(job_skills_set - user_skills_set)\n",
        "\n",
        "                skill_match = len(matching_skills) / max(len(job_skills_set), 1)\n",
        "\n",
        "                model_score = all_model_scores[i]\n",
        "                final_score = 0.7 * model_score + 0.3 * skill_match\n",
        "\n",
        "                recommendations.append({\n",
        "                    'job': job,\n",
        "                    'convfm_score': model_score,\n",
        "                    'skill_match_score': skill_match,\n",
        "                    'final_score': final_score,\n",
        "                    'matching_skills': matching_skills,\n",
        "                    'missing_skills': missing_skills\n",
        "                })\n",
        "\n",
        "\n",
        "        # Sort by final score\n",
        "        recommendations.sort(key=lambda x: x['final_score'], reverse=True)\n",
        "        recommendations = recommendations[:top_k]\n",
        "\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        self._display_evaluation_metrics(recommendations)\n",
        "\n",
        "        # Display results\n",
        "        result = {\n",
        "            'user_profile': user_profile,\n",
        "            'recommendations': recommendations,\n",
        "            'total_jobs_analyzed': len(jobs),\n",
        "            'location': location,\n",
        "            'min_skill_match': min_skill_match,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        self._display_results(result)\n",
        "        self._display_skill_gap_analysis(result)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _display_evaluation_metrics(self, recommendations):\n",
        "        \"\"\"Display ConvFM model evaluation metrics\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üìä ConvFM MODEL EVALUATION METRICS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        if not recommendations:\n",
        "            print(\"No recommendations to evaluate\")\n",
        "            return\n",
        "\n",
        "        convfm_scores = [r['convfm_score'] for r in recommendations]\n",
        "        skill_scores = [r['skill_match_score'] for r in recommendations]\n",
        "        final_scores = [r['final_score'] for r in recommendations]\n",
        "\n",
        "        print(f\"\\nüéØ ConvFM Model Scores:\")\n",
        "        print(f\"   Mean Score: {np.mean(convfm_scores):.4f}\")\n",
        "        print(f\"   Std Dev: {np.std(convfm_scores):.4f}\")\n",
        "        print(f\"   Min Score: {np.min(convfm_scores):.4f}\")\n",
        "        print(f\"   Max Score: {np.max(convfm_scores):.4f}\")\n",
        "\n",
        "        print(f\"\\nüîß Skill Match Scores:\")\n",
        "        print(f\"   Mean Match: {np.mean(skill_scores):.1%}\")\n",
        "        print(f\"   Std Dev: {np.std(skill_scores):.4f}\")\n",
        "        print(f\"   Min Match: {np.min(skill_scores):.1%}\")\n",
        "        print(f\"   Max Match: {np.max(skill_scores):.1%}\")\n",
        "\n",
        "        print(f\"\\nüìà Final Combined Scores (70% ConvFM + 30% Skill Match):\")\n",
        "        print(f\"   Mean Score: {np.mean(final_scores):.4f}\")\n",
        "        print(f\"   Std Dev: {np.std(final_scores):.4f}\")\n",
        "        print(f\"   Min Score: {np.min(final_scores):.4f}\")\n",
        "        print(f\"   Max Score: {np.max(final_scores):.4f}\")\n",
        "\n",
        "        # Precision metrics\n",
        "        high_quality = sum(1 for s in final_scores if s >= 0.7)\n",
        "        medium_quality = sum(1 for s in final_scores if 0.5 <= s < 0.7)\n",
        "        low_quality = sum(1 for s in final_scores if s < 0.5)\n",
        "\n",
        "        print(f\"\\n‚ú® Recommendation Quality Distribution:\")\n",
        "        print(f\"   High Quality (‚â•0.7): {high_quality} jobs ({high_quality/len(recommendations)*100:.1f}%)\")\n",
        "        print(f\"   Medium Quality (0.5-0.7): {medium_quality} jobs ({medium_quality/len(recommendations)*100:.1f}%)\")\n",
        "        print(f\"   Lower Quality (<0.5): {low_quality} jobs ({low_quality/len(recommendations)*100:.1f}%)\")\n",
        "\n",
        "    def _display_results(self, result):\n",
        "        \"\"\"Display recommendations\"\"\"\n",
        "        user = result['user_profile']\n",
        "        recs = result['recommendations']\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üéØ TOP JOB RECOMMENDATIONS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(f\"\\nüë§ CANDIDATE: {user['name']}\")\n",
        "        print(f\"üìß {user['email']}\")\n",
        "        print(f\"üì± {user['phone']}\")\n",
        "        print(f\"üíº {user['experience_years']} years experience\")\n",
        "        print(f\"üéì {user['education_level']}\")\n",
        "        print(f\"üîß {len(user['skills'])} skills\")\n",
        "\n",
        "        print(f\"\\nüìä ANALYSIS:\")\n",
        "        print(f\"   ‚úì Jobs Analyzed: {result['total_jobs_analyzed']}\")\n",
        "        print(f\"   ‚úì Top Matches: {len(recs)}\")\n",
        "        print(f\"   ‚úì Location: {result['location']}\")\n",
        "        print(f\"   ‚úì Min Skill Match: {result['min_skill_match']*100:.0f}%\")\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 80)\n",
        "        print(f\"TOP {len(recs)} JOB RECOMMENDATIONS (Ranked by ConvFM)\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, rec in enumerate(recs, 1):\n",
        "            job = rec['job']\n",
        "\n",
        "            print(f\"\\n{i}. {job['title']}\")\n",
        "            print(f\"üè¢ {job['company']}\")\n",
        "            print(f\"üìç {job['location']} | {job['job_type']}\")\n",
        "            print(f\"üí∞ {job['salary']}\")\n",
        "            print(f\"üìÖ Posted: {job['date_posted']}\")\n",
        "            print(f\"üåê Source: {job['site']}\")\n",
        "            print(f\"\\nüìä Scores:\")\n",
        "            print(f\"   ‚Ä¢ Final Score: {rec['final_score']:.1%}\")\n",
        "            print(f\"   ‚Ä¢ ConvFM Model: {rec['convfm_score']:.1%}\")\n",
        "            print(f\"   ‚Ä¢ Skill Match: {rec['skill_match_score']:.1%}\")\n",
        "\n",
        "            if rec['matching_skills']:\n",
        "                print(f\"\\n‚úÖ Matching Skills ({len(rec['matching_skills'])}): {', '.join(rec['matching_skills'][:8])}\")\n",
        "                if len(rec['matching_skills']) > 8:\n",
        "                    print(f\"       ... and {len(rec['matching_skills']) - 8} more\")\n",
        "\n",
        "            if rec['missing_skills'][:3]:\n",
        "                print(f\"üìö Skills to Learn: {', '.join(rec['missing_skills'][:3])}\")\n",
        "\n",
        "            print(f\"\\nüîó Apply: {job['job_url'][:80]}...\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "    def _display_skill_gap_analysis(self, result):\n",
        "        \"\"\"Display detailed skill gap analysis\"\"\"\n",
        "        user = result['user_profile']\n",
        "        recs = result['recommendations']\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"üìö SKILL GAP ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Collect all required skills\n",
        "        all_required_skills = set()\n",
        "        skill_frequency = Counter()\n",
        "\n",
        "        for rec in recs:\n",
        "            for skill in rec['missing_skills']:\n",
        "                all_required_skills.add(skill)\n",
        "                skill_frequency[skill] += 1\n",
        "\n",
        "        user_skills = set(user['skills'])\n",
        "\n",
        "        print(f\"\\nüìä Overview:\")\n",
        "        print(f\"   Your Skills: {len(user_skills)}\")\n",
        "        print(f\"   Skills in Demand: {len(all_required_skills)}\")\n",
        "        print(f\"   Average Match Rate: {np.mean([r['skill_match_score'] for r in recs]):.1%}\")\n",
        "\n",
        "        if skill_frequency:\n",
        "            print(f\"\\nüéØ TOP 10 SKILLS TO LEARN (by frequency in job postings):\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            for i, (skill, count) in enumerate(skill_frequency.most_common(10), 1):\n",
        "                percentage = (count / len(recs)) * 100\n",
        "                bar = \"‚ñà\" * int(percentage / 5)\n",
        "                print(f\"   {i:2d}. {skill:20s} | {bar:20s} {count:2d}/{len(recs)} jobs ({percentage:5.1f}%)\")\n",
        "        else:\n",
        "            print(\"\\n‚úÖ Great! You have all the skills required for these positions!\")\n",
        "\n",
        "        # Skills you have that are valuable\n",
        "        valuable_skills = Counter()\n",
        "        for rec in recs:\n",
        "            for skill in rec['matching_skills']:\n",
        "                valuable_skills[skill] += 1\n",
        "\n",
        "        if valuable_skills:\n",
        "            print(f\"\\nüíé YOUR MOST VALUABLE SKILLS (appearing in recommendations):\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            for i, (skill, count) in enumerate(valuable_skills.most_common(10), 1):\n",
        "                percentage = (count / len(recs)) * 100\n",
        "                bar = \"‚ñà\" * int(percentage / 5)\n",
        "                print(f\"   {i:2d}. {skill:20s} | {bar:20s} {count:2d}/{len(recs)} jobs ({percentage:5.1f}%)\")\n",
        "\n",
        "        # Learning path recommendation\n",
        "        print(f\"\\nüéì RECOMMENDED LEARNING PATH:\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        top_skills = [skill for skill, _ in skill_frequency.most_common(5)]\n",
        "\n",
        "        if top_skills:\n",
        "            print(\"Based on your job matches, focus on learning these skills in order:\")\n",
        "            for i, skill in enumerate(top_skills, 1):\n",
        "                count = skill_frequency[skill]\n",
        "                print(f\"   {i}. {skill} - Required by {count} of your top matches\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ You're well-prepared for these positions!\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)"
      ],
      "metadata": {
        "id": "FW80hfL6ZVCv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FILE UPLOAD INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "def upload_resume():\n",
        "    \"\"\"Interactive resume upload in Colab\"\"\"\n",
        "    if not COLAB_ENV:\n",
        "        print(\"‚ö†Ô∏è  Not in Colab. Use: recommend_from_resume('path/to/resume.pdf', 'pdf')\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üì§ UPLOAD YOUR RESUME\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Supported formats: PDF, DOCX, TXT\")\n",
        "    print(\"Click 'Choose Files' button below...\\n\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No file uploaded!\")\n",
        "        return None, None\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    file_ext = filename.split('.')[-1].lower()\n",
        "\n",
        "    print(f\"\\n‚úÖ Uploaded: {filename}\")\n",
        "    print(f\"üìã File type: {file_ext}\")\n",
        "    print(f\"üì¶ Size: {len(uploaded[filename]) / 1024:.1f} KB\")\n",
        "\n",
        "    return filename, file_ext\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ ConvFM JOB RECOMMENDATION SYSTEM READY!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüöÄ USAGE:\")\n",
        "\n",
        "# Step 1: Upload your resume\n",
        "filename, file_type = upload_resume()\n",
        "\n",
        "# Step 2: Get recommendations\n",
        "engine = ConvFMJobRecommender()\n",
        "\n",
        "result = engine.recommend_from_resume(\n",
        "    resume_path=filename,\n",
        "    resume_type=file_type,\n",
        "    location='India',          # Specify location\n",
        "    num_jobs=100,              # Number of jobs to analyze\n",
        "    top_k=15,                  # Number of recommendations\n",
        "    min_skill_match=0.3,       # Minimum 30% skill match\n",
        "    site_names=['indeed', 'linkedin']\n",
        ")\n",
        "\n",
        "# Step 3: Save results\n",
        "if result:\n",
        "    with open('job_recommendations.json', 'w') as f:\n",
        "        json.dump(result, f, indent=2, default=str)\n",
        "    print(\"‚úÖ Results saved to job_recommendations.json\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Key Features:\")\n",
        "print(\"  ‚úì Uses ConvFM neural network for intelligent matching\")\n",
        "print(\"  ‚úì Real-time job scraping from Indeed & LinkedIn\")\n",
        "print(\"  ‚úì Skills-based matching (minimum 30% threshold)\")\n",
        "print(\"  ‚úì Location-based filtering\")\n",
        "print(\"  ‚úì Detailed evaluation metrics\")\n",
        "print(\"  ‚úì Comprehensive skill gap analysis\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sNnew4uOZYkc",
        "outputId": "60a8bc3c-f4fa-4a76-edd3-c31d9e6caf0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "‚úÖ ConvFM JOB RECOMMENDATION SYSTEM READY!\n",
            "================================================================================\n",
            "\n",
            "üöÄ USAGE:\n",
            "================================================================================\n",
            "üì§ UPLOAD YOUR RESUME\n",
            "================================================================================\n",
            "Supported formats: PDF, DOCX, TXT\n",
            "Click 'Choose Files' button below...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a9f20257-f3a1-4ad0-8f6d-df43c8b231af\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a9f20257-f3a1-4ad0-8f6d-df43c8b231af\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving KiranDhanvate-resume.pdf to KiranDhanvate-resume (3).pdf\n",
            "\n",
            "‚úÖ Uploaded: KiranDhanvate-resume (3).pdf\n",
            "üìã File type: pdf\n",
            "üì¶ Size: 4042.3 KB\n",
            "\n",
            "================================================================================\n",
            "üöÄ STARTING ConvFM JOB RECOMMENDATION\n",
            "================================================================================\n",
            "\n",
            "üìÑ Parsing resume: KiranDhanvate-resume (3).pdf\n",
            "============================================================\n",
            "‚úÖ Successfully parsed resume!\n",
            "\n",
            "Name: Kiran Dhanvate\n",
            "Email: kirandhanvate735@gmail.com\n",
            "Phone: +91 9373791110\n",
            "Skills Found: 28\n",
            "Experience: 0 years\n",
            "Education: Bachelors\n",
            "\n",
            "üîß Top Skills: Azure, C, CI/CD, Computer Vision, Deep Learning, Django, Docker, FastAPI, Flask, Git\n",
            "       ... and 18 more\n",
            "\n",
            "üîç SCRAPING REAL-TIME JOBS\n",
            "============================================================\n",
            "   Location: India\n",
            "   Sites: indeed, linkedin\n",
            "   Target: 100 jobs\n",
            "\n",
            "‚úÖ Successfully scraped 113 REAL jobs!\n",
            "üìä Breakdown:\n",
            "      - indeed: 59 jobs\n",
            "      - linkedin: 54 jobs\n",
            "\n",
            "üîß PROCESSING 113 JOBS\n",
            "============================================================\n",
            "‚úÖ Processed 113 jobs successfully\n",
            "üìä Skills per job: avg 2.4, max 23\n",
            "\n",
            "================================================================================\n",
            "üß† INITIALIZING ConvFM MODEL\n",
            "================================================================================\n",
            "\n",
            "üìö Building vocabulary...\n",
            "‚úÖ Vocabulary size: 1341\n",
            "\n",
            "üèóÔ∏è Building ConvFM model...\n",
            "‚úÖ Model initialized with 324,097 parameters\n",
            "\n",
            "üéØ Quick training ConvFM...\n",
            "   Epoch 1/15 - Loss: 5.6638\n",
            "   Epoch 2/15 - Loss: 4.2060\n",
            "   Epoch 3/15 - Loss: 2.8831\n",
            "   Epoch 4/15 - Loss: 1.7256\n",
            "   Epoch 5/15 - Loss: 5.1220\n",
            "   Epoch 6/15 - Loss: 2.4487\n",
            "   Epoch 7/15 - Loss: 1.6935\n",
            "   Epoch 8/15 - Loss: 4.0330\n",
            "   Epoch 9/15 - Loss: 1.8454\n",
            "   Epoch 10/15 - Loss: 1.8767\n",
            "   Epoch 11/15 - Loss: 2.2265\n",
            "   Epoch 12/15 - Loss: 1.7713\n",
            "   Epoch 13/15 - Loss: 3.0261\n",
            "   Epoch 14/15 - Loss: 1.7534\n",
            "   Epoch 15/15 - Loss: 1.7151\n",
            "‚úÖ Model training complete!\n",
            "\n",
            "================================================================================\n",
            "üéØ GENERATING RECOMMENDATIONS WITH ConvFM\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring jobs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 16.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üìä ConvFM MODEL EVALUATION METRICS\n",
            "================================================================================\n",
            "\n",
            "üéØ ConvFM Model Scores:\n",
            "   Mean Score: 1.0000\n",
            "   Std Dev: 0.0000\n",
            "   Min Score: 1.0000\n",
            "   Max Score: 1.0000\n",
            "\n",
            "üîß Skill Match Scores:\n",
            "   Mean Match: 63.2%\n",
            "   Std Dev: 0.0919\n",
            "   Min Match: 50.0%\n",
            "   Max Match: 83.3%\n",
            "\n",
            "üìà Final Combined Scores (70% ConvFM + 30% Skill Match):\n",
            "   Mean Score: 0.8897\n",
            "   Std Dev: 0.0276\n",
            "   Min Score: 0.8500\n",
            "   Max Score: 0.9500\n",
            "\n",
            "‚ú® Recommendation Quality Distribution:\n",
            "   High Quality (‚â•0.7): 15 jobs (100.0%)\n",
            "   Medium Quality (0.5-0.7): 0 jobs (0.0%)\n",
            "   Lower Quality (<0.5): 0 jobs (0.0%)\n",
            "\n",
            "================================================================================\n",
            "üéØ TOP JOB RECOMMENDATIONS\n",
            "================================================================================\n",
            "\n",
            "üë§ CANDIDATE: Kiran Dhanvate\n",
            "üìß kirandhanvate735@gmail.com\n",
            "üì± +91 9373791110\n",
            "üíº 0 years experience\n",
            "üéì Bachelors\n",
            "üîß 28 skills\n",
            "\n",
            "üìä ANALYSIS:\n",
            "   ‚úì Jobs Analyzed: 113\n",
            "   ‚úì Top Matches: 15\n",
            "   ‚úì Location: India\n",
            "   ‚úì Min Skill Match: 30%\n",
            "\n",
            "================================================================================\n",
            "TOP 15 JOB RECOMMENDATIONS (Ranked by ConvFM)\n",
            "================================================================================\n",
            "\n",
            "1. Sr. Dot Net Developer\n",
            "üè¢ Whitelotus Corporation Pvt Ltd\n",
            "üìç Remote, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-10-31\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 95.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 83.3%\n",
            "\n",
            "‚úÖ Matching Skills (5): Kubernetes, Git, Docker, PostgreSQL, C\n",
            "üìö Skills to Learn: Microservices\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=e910fb1c9f5aa05c...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. Sr. QA Automation Engineer\n",
            "üè¢ teknobloom consulting\n",
            "üìç UP, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 92.5%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 75.0%\n",
            "\n",
            "‚úÖ Matching Skills (6): Python, Azure, Git, Java, GitHub, CI/CD\n",
            "üìö Skills to Learn: REST API, Jenkins\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=af7466c898f8b3d6...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. FDDM_Python Developer\n",
            "üè¢ BMW TechWorks India\n",
            "üìç KA, IN | nan\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 91.8%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 72.7%\n",
            "\n",
            "‚úÖ Matching Skills (8): Python, Kubernetes, FastAPI, Docker, GitHub, Django, PostgreSQL, CI/CD\n",
            "üìö Skills to Learn: AWS, JIRA, Pandas\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=2f1e5a4e0ab10372...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. Backend Developer( 6+ Years )\n",
            "üè¢ Infomagine softwares pvt ltd\n",
            "üìç RJ, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 91.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 70.0%\n",
            "\n",
            "‚úÖ Matching Skills (7): Python, Kubernetes, MySQL, MongoDB, Azure, Docker, PostgreSQL\n",
            "üìö Skills to Learn: Microservices, Node.js, AWS\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=f0a0c29124dff09e...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. Software Developer\n",
            "üè¢ Voicene Technologies LLP\n",
            "üìç KL, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 90.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 66.7%\n",
            "\n",
            "‚úÖ Matching Skills (4): React, JavaScript, Git, C\n",
            "üìö Skills to Learn: Node.js, WebSocket\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=1713f705dac829b3...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "6. FDDM_Java Backend Junior Developer\n",
            "üè¢ BMW TechWorks India\n",
            "üìç KA, IN | nan\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 90.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 66.7%\n",
            "\n",
            "‚úÖ Matching Skills (8): Python, Kubernetes, Linux, MongoDB, Java, Docker, GitHub, CI/CD\n",
            "üìö Skills to Learn: Oracle, Terraform, JIRA\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=500b0a40d2c57db1...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "7. Developer-Postgre SQL Database (5+ years)\n",
            "üè¢ SAP\n",
            "üìç KA, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 90.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 66.7%\n",
            "\n",
            "‚úÖ Matching Skills (6): Python, Linux, Azure, Java, PostgreSQL, CI/CD\n",
            "üìö Skills to Learn: Unix, GCP, AWS\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=6841392754d3baec...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "8. Lead Full Stack Developer (Ahmedabad Exp. 5 Years)\n",
            "üè¢ Praeclarum Tech\n",
            "üìç GJ, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 88.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 60.0%\n",
            "\n",
            "‚úÖ Matching Skills (3): MongoDB, JavaScript, React\n",
            "üìö Skills to Learn: TypeScript, Node.js\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=bbd9a8eb911ad532...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "9. Java Developer - Cloud-Native Microservices & Observability\n",
            "üè¢ SAP\n",
            "üìç KA, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 88.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 60.0%\n",
            "\n",
            "‚úÖ Matching Skills (6): Python, Kubernetes, Azure, Java, Docker, CI/CD\n",
            "üìö Skills to Learn: Kafka, Microservices, GCP\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=5aa017fe25730acf...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "10. Senior ABAP Developer - ERP PCP Logistics & Procurement\n",
            "üè¢ SAP\n",
            "üìç KA, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 88.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 60.0%\n",
            "\n",
            "‚úÖ Matching Skills (3): JavaScript, Java, C\n",
            "üìö Skills to Learn: Agile, CSS\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=dffb7d62c01a7319...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "11. Experienced MERN Stack Developer\n",
            "üè¢ Patoliya Infotech\n",
            "üìç GJ, IN | fulltime\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 87.1%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 57.1%\n",
            "\n",
            "‚úÖ Matching Skills (4): React, MongoDB, JavaScript, Git\n",
            "üìö Skills to Learn: Express.js, Node.js, Redux\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=a005a0576dd87759...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "12. Software QA Automation Engineer\n",
            "üè¢ AppDirect\n",
            "üìç MH, IN | nan\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 86.7%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 55.6%\n",
            "\n",
            "‚úÖ Matching Skills (5): MySQL, Linux, MongoDB, Git, Docker\n",
            "üìö Skills to Learn: Unix, Agile, JIRA\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=8de91c1b9a62cdcc...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "13. FDDM_Front_End_Developer\n",
            "üè¢ BMW TechWorks India\n",
            "üìç KA, IN | nan\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 86.4%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 54.5%\n",
            "\n",
            "‚úÖ Matching Skills (6): Kubernetes, CI/CD, Docker, GitHub, PostgreSQL, JavaScript\n",
            "üìö Skills to Learn: JIRA, HTML, Angular\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=125f5baf2a8ebf1d...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "14. UI/Front End Developer\n",
            "üè¢ Womennnovator\n",
            "üìç DL, IN | nan\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 85.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 50.0%\n",
            "\n",
            "‚úÖ Matching Skills (2): React, JavaScript\n",
            "üìö Skills to Learn: CSS, Angular\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=60d4bb179bccaacc...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "15. Software Engineering Manager, Key-Value Engine\n",
            "üè¢ Couchbase, Inc.\n",
            "üìç KA, IN | nan\n",
            "üí∞ Not specified\n",
            "üìÖ Posted: 2025-11-03\n",
            "üåê Source: indeed\n",
            "\n",
            "üìä Scores:\n",
            "   ‚Ä¢ Final Score: 85.0%\n",
            "   ‚Ä¢ ConvFM Model: 100.0%\n",
            "   ‚Ä¢ Skill Match: 50.0%\n",
            "\n",
            "‚úÖ Matching Skills (1): C\n",
            "üìö Skills to Learn: LESS\n",
            "\n",
            "üîó Apply: https://in.indeed.com/viewjob?jk=2a35a6f0630bf526...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üìö SKILL GAP ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "üìä Overview:\n",
            "   Your Skills: 28\n",
            "   Skills in Demand: 21\n",
            "   Average Match Rate: 63.2%\n",
            "\n",
            "üéØ TOP 10 SKILLS TO LEARN (by frequency in job postings):\n",
            "--------------------------------------------------------------------------------\n",
            "    1. AWS                  | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                5/15 jobs ( 33.3%)\n",
            "    2. JIRA                 | ‚ñà‚ñà‚ñà‚ñà‚ñà                 4/15 jobs ( 26.7%)\n",
            "    3. Node.js              | ‚ñà‚ñà‚ñà‚ñà‚ñà                 4/15 jobs ( 26.7%)\n",
            "    4. Microservices        | ‚ñà‚ñà‚ñà‚ñà                  3/15 jobs ( 20.0%)\n",
            "    5. CSS                  | ‚ñà‚ñà‚ñà‚ñà                  3/15 jobs ( 20.0%)\n",
            "    6. Jenkins              | ‚ñà‚ñà                    2/15 jobs ( 13.3%)\n",
            "    7. Unix                 | ‚ñà‚ñà                    2/15 jobs ( 13.3%)\n",
            "    8. GCP                  | ‚ñà‚ñà                    2/15 jobs ( 13.3%)\n",
            "    9. TypeScript           | ‚ñà‚ñà                    2/15 jobs ( 13.3%)\n",
            "   10. Agile                | ‚ñà‚ñà                    2/15 jobs ( 13.3%)\n",
            "\n",
            "üíé YOUR MOST VALUABLE SKILLS (appearing in recommendations):\n",
            "--------------------------------------------------------------------------------\n",
            "    1. Docker               | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             7/15 jobs ( 46.7%)\n",
            "    2. Kubernetes           | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              6/15 jobs ( 40.0%)\n",
            "    3. Python               | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              6/15 jobs ( 40.0%)\n",
            "    4. CI/CD                | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              6/15 jobs ( 40.0%)\n",
            "    5. JavaScript           | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              6/15 jobs ( 40.0%)\n",
            "    6. Git                  | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                5/15 jobs ( 33.3%)\n",
            "    7. PostgreSQL           | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                5/15 jobs ( 33.3%)\n",
            "    8. Java                 | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                5/15 jobs ( 33.3%)\n",
            "    9. MongoDB              | ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                5/15 jobs ( 33.3%)\n",
            "   10. C                    | ‚ñà‚ñà‚ñà‚ñà‚ñà                 4/15 jobs ( 26.7%)\n",
            "\n",
            "üéì RECOMMENDED LEARNING PATH:\n",
            "--------------------------------------------------------------------------------\n",
            "Based on your job matches, focus on learning these skills in order:\n",
            "   1. AWS - Required by 5 of your top matches\n",
            "   2. JIRA - Required by 4 of your top matches\n",
            "   3. Node.js - Required by 4 of your top matches\n",
            "   4. Microservices - Required by 3 of your top matches\n",
            "   5. CSS - Required by 3 of your top matches\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Results saved to job_recommendations.json\n",
            "\n",
            "================================================================================\n",
            "Key Features:\n",
            "  ‚úì Uses ConvFM neural network for intelligent matching\n",
            "  ‚úì Real-time job scraping from Indeed & LinkedIn\n",
            "  ‚úì Skills-based matching (minimum 30% threshold)\n",
            "  ‚úì Location-based filtering\n",
            "  ‚úì Detailed evaluation metrics\n",
            "  ‚úì Comprehensive skill gap analysis\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data and run predictions\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Use preprocessor to transform features\n",
        "features_np, _ = preprocessor.transform(df_eval)\n",
        "features = torch.LongTensor(features_np).to(device)\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(features), 1024):\n",
        "        batch = features[i:i+1024]\n",
        "        out = model(batch)\n",
        "        preds.append(out.detach().cpu().numpy())\n",
        "\n",
        "preds = np.concatenate(preds).reshape(-1)\n",
        "\n",
        "# Attach predictions to dataframe\n",
        "output_df = df_eval.copy()\n",
        "output_df['prediction'] = preds\n",
        "\n",
        "# Print same-style output + metrics (if label available)\n",
        "print(\"\\nSAMPLE OUTPUT (first 5 rows):\")\n",
        "print(output_df.head()[FEATURE_COLUMNS + ([TARGET_COLUMN] if TARGET_COLUMN else []) + ['prediction']])\n",
        "\n",
        "if TARGET_COLUMN is not None:\n",
        "    y_true = df_eval[TARGET_COLUMN].values\n",
        "    # If target is binary {0,1}, compute classification metrics\n",
        "    if set(np.unique(y_true)).issubset({0,1}):\n",
        "        prob = 1 / (1 + np.exp(-preds))  # if logits; if already probs, it's okay\n",
        "        y_pred = (prob > 0.5).astype(int)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true, prob)\n",
        "        except:\n",
        "            auc = float('nan')\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        print(\"\\nMETRICS (Binary Classification):\")\n",
        "        print(f\"  AUC:        {auc:.6f}\")\n",
        "        print(f\"  Accuracy:   {acc:.6f}\")\n",
        "        print(f\"  Precision:  {prec:.6f}\")\n",
        "        print(f\"  Recall:     {rec:.6f}\")\n",
        "        print(f\"  F1-Score:   {f1:.6f}\")\n",
        "    else:\n",
        "        # Regression metrics\n",
        "        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "        mse = mean_squared_error(y_true, preds)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_true, preds)\n",
        "        r2 = r2_score(y_true, preds)\n",
        "        print(\"\\nMETRICS (Regression):\")\n",
        "        print(f\"  MSE:   {mse:.6f}\")\n",
        "        print(f\"  RMSE:  {rmse:.6f}\")\n",
        "        print(f\"  MAE:   {mae:.6f}\")\n",
        "        print(f\"  R^2:   {r2:.6f}\")\n",
        "\n",
        "# Save predictions\n",
        "output_path = 'convfm_predictions.csv'\n",
        "output_df.to_csv(output_path, index=False)\n",
        "print(f\"\\n‚úì Predictions saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0DzBiE6zZYn4",
        "outputId": "26e66ad5-0593-4a34-fe5e-0f2992d26972"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocessor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2240149165.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Use preprocessor to transform features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfeatures_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YdJ5UTCZTbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
