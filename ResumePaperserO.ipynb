{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0inivaXi2UO",
        "outputId": "19394969-7710-4d94-b486-f6868a6a6362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 46.0 kB/128 kB 36%] [3 InRelease 60.5 kB/129 kB 47%] [Connected\r0% [2 InRelease 128 kB/128 kB 100%] [Connected to cloud.r-project.org (65.9.86.\r0% [Connected to cloud.r-project.org (65.9.86.109)] [Connecting to r2u.stat.ill\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "\r0% [4 InRelease 47.5 kB/127 kB 37%] [Connected to cloud.r-project.org (65.9.86.\r0% [Waiting for headers] [Connecting to r2u.stat.illinois.edu] [Waiting for hea\r                                                                               \rHit:5 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connecting to r2u.stat.illinois.edu] [Waiting for hea\r                                                                               \rGet:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                               \rGet:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "\r0% [Connecting to r2u.stat.illinois.edu (192.17.190.167)] [8 InRelease 16.0 kB/\r0% [Connecting to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers]\r                                                                               \rHit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,276 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,425 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,751 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,584 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,077 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,354 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,813 kB]\n",
            "Fetched 24.7 MB in 3s (7,884 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Collecting Levenshtein==0.27.1 (from python-levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-levenshtein)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six) (2.0.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.23)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, pytesseract, PyMuPDF, pdf2image, Levenshtein, python-levenshtein, pdfminer.six\n",
            "Successfully installed Levenshtein-0.27.1 PyMuPDF-1.26.5 fuzzywuzzy-0.18.0 pdf2image-1.17.0 pdfminer.six-20250506 pytesseract-0.3.13 python-levenshtein-0.27.1 rapidfuzz-3.14.1\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m130.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "âœ… All dependencies installed and imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pdfminer.six pytesseract pdf2image PyMuPDF spacy fuzzywuzzy python-levenshtein\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Import all libraries\n",
        "import fitz  # PyMuPDF\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from pdfminer.high_level import extract_text\n",
        "import spacy\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "from fuzzywuzzy import fuzz\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… All dependencies installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxiBmFrCi3zt",
        "outputId": "0c56e16b-c79b-496d-8b42-4568c3999b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Text extraction function defined!\n"
          ]
        }
      ],
      "source": [
        "def extract_text_from_resume(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text from PDF resumes. Automatically detects if PDF is\n",
        "    scanned/image-based and applies appropriate extraction method.\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“„ Processing: {file_path}\")\n",
        "\n",
        "    def is_scanned_pdf(pdf_path):\n",
        "        \"\"\"Check if PDF is scanned by analyzing text content ratio\"\"\"\n",
        "        try:\n",
        "            # First try native text extraction\n",
        "            doc = fitz.open(pdf_path)\n",
        "            total_text = \"\"\n",
        "            total_images = 0\n",
        "\n",
        "            for page in doc:\n",
        "                total_text += page.get_text()\n",
        "                total_images += len(page.get_images())\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            # If very little text but images exist, likely scanned\n",
        "            text_length = len(total_text.strip())\n",
        "            is_scanned = (text_length < 100 and total_images > 0) or text_length < 50\n",
        "\n",
        "            print(f\"   ğŸ“Š Text length: {text_length}, Images: {total_images}\")\n",
        "            print(f\"   ğŸ” PDF Type: {'SCANNED' if is_scanned else 'DIGITAL'}\")\n",
        "\n",
        "            return is_scanned, total_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âš ï¸ Error analyzing PDF: {str(e)}\")\n",
        "            return True, \"\"  # Default to OCR if uncertain\n",
        "\n",
        "    def extract_with_ocr(pdf_path):\n",
        "        \"\"\"Extract text using OCR for scanned PDFs\"\"\"\n",
        "        print(\"   ğŸ”„ Applying OCR (Tesseract)...\")\n",
        "        try:\n",
        "            # Convert PDF to images\n",
        "            pages = convert_from_path(pdf_path, dpi=300)\n",
        "            extracted_text = \"\"\n",
        "\n",
        "            for i, page in enumerate(pages):\n",
        "                print(f\"      Processing page {i+1}/{len(pages)}\")\n",
        "                # OCR configuration for better accuracy\n",
        "                custom_config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz.,@+-()[]{}|/:;!?\"\\' '\n",
        "                page_text = pytesseract.image_to_string(page, config=custom_config)\n",
        "                extracted_text += page_text + \"\\n\"\n",
        "\n",
        "            return extracted_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ OCR failed: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def extract_digital_text(pdf_path):\n",
        "        \"\"\"Extract text from digital PDFs\"\"\"\n",
        "        print(\"   ğŸ“– Extracting digital text...\")\n",
        "        try:\n",
        "            # Try PyMuPDF first\n",
        "            text = \"\"\n",
        "            doc = fitz.open(pdf_path)\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            doc.close()\n",
        "\n",
        "            # Fallback to pdfminer if PyMuPDF gives little content\n",
        "            if len(text.strip()) < 100:\n",
        "                print(\"   ğŸ”„ Fallback to pdfminer...\")\n",
        "                text = extract_text(pdf_path)\n",
        "\n",
        "            return text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Digital extraction failed: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    # Main extraction logic\n",
        "    try:\n",
        "        is_scanned, initial_text = is_scanned_pdf(file_path)\n",
        "\n",
        "        if is_scanned:\n",
        "            final_text = extract_with_ocr(file_path)\n",
        "        else:\n",
        "            final_text = initial_text if initial_text else extract_digital_text(file_path)\n",
        "\n",
        "        if len(final_text.strip()) < 50:\n",
        "            print(\"   âš ï¸ Low text yield, attempting OCR fallback...\")\n",
        "            final_text = extract_with_ocr(file_path)\n",
        "\n",
        "        print(f\"âœ… Extraction complete: {len(final_text)} characters\")\n",
        "        return final_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Extraction failed: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"âœ… Text extraction function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AwuQMm3i_vb",
        "outputId": "edc4fda3-1f4d-4434-cb3d-71fc7cc78943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Text cleaning function defined!\n"
          ]
        }
      ],
      "source": [
        "def clean_and_normalize_text(raw_text):\n",
        "    \"\"\"\n",
        "    Cleans and normalizes extracted text for better parsing.\n",
        "    Removes headers, footers, excessive whitespace, and special characters.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§¹ Cleaning and normalizing text...\")\n",
        "\n",
        "    if not raw_text:\n",
        "        return \"\"\n",
        "\n",
        "    text = raw_text\n",
        "\n",
        "    # Remove common headers/footers patterns\n",
        "    header_footer_patterns = [\n",
        "        r'Page \\d+ of \\d+',\n",
        "        r'www\\.[\\w\\-\\.]+\\.com',\n",
        "        r'Â©.*?\\d{4}',\n",
        "        r'Confidential.*',\n",
        "        r'Resume.*Page.*\\d+',\n",
        "    ]\n",
        "\n",
        "    for pattern in header_footer_patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Normalize whitespace and line breaks\n",
        "    text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)  # Multiple line breaks to double\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces/tabs to single space\n",
        "    text = re.sub(r'\\n ', '\\n', text)  # Remove spaces at line starts\n",
        "\n",
        "    # Remove excessive special characters but keep important ones\n",
        "    text = re.sub(r'[^\\w\\s@.+\\-():,/\\\\|&%#\\n]', '', text)\n",
        "\n",
        "    # Remove very short lines that are likely artifacts\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) > 1:  # Keep lines with more than 1 character\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "    cleaned_text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "    print(f\"   âœ… Cleaned text length: {len(cleaned_text)} chars\")\n",
        "    return cleaned_text\n",
        "\n",
        "print(\"âœ… Text cleaning function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "envtBl4PjC6d"
      },
      "outputs": [],
      "source": [
        "def extract_skills(text):\n",
        "    \"\"\"\n",
        "    COMPREHENSIVE skills extraction that handles parentheses, special characters,\n",
        "    and complex formatting like 'SQL(Oracle, MySQL)', 'MongoDB(Basic)' etc.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ› ï¸ Extracting skills with ADVANCED parsing...\")\n",
        "\n",
        "    # COMPREHENSIVE Technical Skills Database\n",
        "    TECHNICAL_SKILLS = [\n",
        "        # Programming Languages\n",
        "        'C++', 'C#', 'C', 'Python', 'Java', 'JavaScript', 'TypeScript', 'PHP', 'Ruby',\n",
        "        'Swift', 'Kotlin', 'Go', 'Rust', 'Scala', 'R', 'MATLAB', 'Dart', 'Perl', 'SQL',\n",
        "\n",
        "        # Web Technologies\n",
        "        'HTML', 'CSS', 'React', 'Angular', 'Vue', 'Node.js', 'Express', 'Django', 'Flask',\n",
        "        'Spring', 'Bootstrap', 'Tailwind CSS', 'jQuery', 'SASS', 'LESS', 'REST API', 'GraphQL',\n",
        "\n",
        "        # Databases & Database Technologies\n",
        "        'MySQL', 'PostgreSQL', 'MongoDB', 'Redis', 'Oracle', 'SQLite', 'NoSQL', 'Cassandra',\n",
        "        'Firebase', 'DynamoDB', 'MariaDB', 'Neo4j', 'Elasticsearch', 'SQL Server',\n",
        "\n",
        "        # Data Science & ML\n",
        "        'Scikit-learn', 'Pandas', 'NumPy', 'Matplotlib', 'Seaborn', 'TensorFlow', 'PyTorch',\n",
        "        'Keras', 'OpenCV', 'NLTK', 'Spark', 'Hadoop',\n",
        "\n",
        "        # Development Tools & Version Control\n",
        "        'Git', 'GitHub', 'GitLab', 'Bitbucket', 'SVN', 'VS Code', 'Visual Studio Code',\n",
        "        'PyCharm', 'IntelliJ', 'Eclipse', 'Android Studio', 'Xcode',\n",
        "\n",
        "        # IDEs & Editors\n",
        "        'Jupyter', 'Jupyter Notebook', 'Google Colab', 'Anaconda', 'Atom', 'Sublime Text',\n",
        "\n",
        "        # Cloud Platforms\n",
        "        'AWS', 'Azure', 'Google Cloud', 'GCP', 'Docker', 'Kubernetes', 'Jenkins',\n",
        "\n",
        "        # Mobile Development\n",
        "        'Android', 'iOS', 'React Native', 'Flutter', 'Xamarin', 'Ionic',\n",
        "\n",
        "        # Operating Systems\n",
        "        'Linux', 'Windows', 'MacOS', 'Unix', 'Ubuntu',\n",
        "\n",
        "        # Libraries & Frameworks\n",
        "        'React.js', 'Angular.js', 'Vue.js', 'Express.js', 'Next.js', 'Nuxt.js'\n",
        "    ]\n",
        "\n",
        "    # Skill variations and aliases\n",
        "    SKILL_VARIANTS = {\n",
        "        'C++': ['C plus plus', 'Cpp', 'CPP', 'c++'],\n",
        "        'C#': ['C sharp', 'CSharp', 'c#'],\n",
        "        'JavaScript': ['JS', 'Javascript', 'ECMAScript', 'js'],\n",
        "        'React': ['React.js', 'ReactJS'],\n",
        "        'Node.js': ['NodeJS', 'Node'],\n",
        "        'Tailwind CSS': ['Tailwind', 'TailwindCSS'],\n",
        "        'Visual Studio Code': ['VS Code', 'VSCode'],\n",
        "        'Jupyter Notebook': ['Jupyter'],\n",
        "        'GitHub': ['Github'],\n",
        "        'MongoDB': ['Mongo'],\n",
        "        'PostgreSQL': ['Postgres'],\n",
        "        'Scikit-learn': ['sklearn', 'scikit learn']\n",
        "    }\n",
        "\n",
        "    def preprocess_skills_text(text):\n",
        "        \"\"\"\n",
        "        Preprocess text to extract skills from complex formats like:\n",
        "        'SQL(Oracle, MySQL)', 'MongoDB(Basic)', 'React.js (Library)'\n",
        "        \"\"\"\n",
        "        processed_text = text\n",
        "\n",
        "        # Extract content from parentheses and add as separate items\n",
        "        # Pattern: Word(content) -> Word, content\n",
        "        parentheses_pattern = r'(\\w+)\\s*\\(([^)]+)\\)'\n",
        "        matches = re.findall(parentheses_pattern, processed_text)\n",
        "\n",
        "        for main_item, inside_content in matches:\n",
        "            # Add the main item\n",
        "            processed_text += f\" {main_item} \"\n",
        "\n",
        "            # Add items inside parentheses (split by comma)\n",
        "            inside_items = [item.strip() for item in inside_content.split(',')]\n",
        "            for item in inside_items:\n",
        "                # Clean item (remove words like 'Basic', 'Library', 'Framework')\n",
        "                clean_item = re.sub(r'\\b(Basic|Library|Framework|Advanced|Intermediate)\\b', '', item, flags=re.IGNORECASE).strip()\n",
        "                if clean_item and len(clean_item) > 1:\n",
        "                    processed_text += f\" {clean_item} \"\n",
        "\n",
        "        return processed_text\n",
        "\n",
        "    def extract_skills_with_advanced_matching(text):\n",
        "        \"\"\"Extract skills using multiple strategies\"\"\"\n",
        "        found_skills = set()\n",
        "\n",
        "        # Preprocess text to handle parentheses\n",
        "        processed_text = preprocess_skills_text(text)\n",
        "        text_for_search = ' ' + processed_text.lower() + ' '\n",
        "\n",
        "        for skill in TECHNICAL_SKILLS:\n",
        "            skill_found = False\n",
        "\n",
        "            # Strategy 1: Exact match with word boundaries\n",
        "            skill_lower = skill.lower()\n",
        "\n",
        "            # Handle special characters in skill names\n",
        "            if any(char in skill for char in ['+', '#', '.', '-']):\n",
        "                # For C++, C#, React.js etc.\n",
        "                escaped_skill = re.escape(skill_lower)\n",
        "                patterns_to_try = [\n",
        "                    r'\\b' + escaped_skill + r'\\b',\n",
        "                    r'(?<!\\w)' + escaped_skill + r'(?!\\w)',  # Alternative boundary\n",
        "                    escaped_skill  # Fallback without boundaries\n",
        "                ]\n",
        "\n",
        "                for pattern in patterns_to_try:\n",
        "                    if re.search(pattern, text_for_search):\n",
        "                        found_skills.add(skill)\n",
        "                        skill_found = True\n",
        "                        break\n",
        "            else:\n",
        "                # Standard word boundary matching\n",
        "                pattern = r'\\b' + re.escape(skill_lower) + r'\\b'\n",
        "                if re.search(pattern, text_for_search):\n",
        "                    found_skills.add(skill)\n",
        "                    skill_found = True\n",
        "\n",
        "            # Strategy 2: Check variants if main skill not found\n",
        "            if not skill_found and skill in SKILL_VARIANTS:\n",
        "                for variant in SKILL_VARIANTS[skill]:\n",
        "                    variant_lower = variant.lower()\n",
        "                    if any(char in variant for char in ['+', '#', '.', '-']):\n",
        "                        escaped_variant = re.escape(variant_lower)\n",
        "                        variant_patterns = [\n",
        "                            r'\\b' + escaped_variant + r'\\b',\n",
        "                            r'(?<!\\w)' + escaped_variant + r'(?!\\w)',\n",
        "                            escaped_variant\n",
        "                        ]\n",
        "\n",
        "                        for vpattern in variant_patterns:\n",
        "                            if re.search(vpattern, text_for_search):\n",
        "                                found_skills.add(skill)\n",
        "                                skill_found = True\n",
        "                                break\n",
        "                    else:\n",
        "                        variant_pattern = r'\\b' + re.escape(variant_lower) + r'\\b'\n",
        "                        if re.search(variant_pattern, text_for_search):\n",
        "                            found_skills.add(skill)\n",
        "                            skill_found = True\n",
        "                            break\n",
        "\n",
        "                    if skill_found:\n",
        "                        break\n",
        "\n",
        "        return found_skills\n",
        "\n",
        "    def locate_skills_sections(text):\n",
        "        \"\"\"Find all skills-related sections\"\"\"\n",
        "        section_patterns = [\n",
        "            r'(?i)skills?\\s*:?',\n",
        "            r'(?i)languages?\\s*:',\n",
        "            r'(?i)database\\s*:',\n",
        "            r'(?i)dev[-\\s]?tools?\\s*:',\n",
        "            r'(?i)technologies?\\s*:',\n",
        "            r'(?i)libraries?/?frameworks?\\s*:',\n",
        "            r'(?i)frameworks?\\s*:',\n",
        "            r'(?i)tools?\\s*:',\n",
        "            r'(?i)programming\\s+languages?\\s*:',\n",
        "            r'(?i)technical\\s+skills?\\s*:'\n",
        "        ]\n",
        "\n",
        "        skills_content = []\n",
        "\n",
        "        for pattern in section_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                start_pos = match.end()\n",
        "\n",
        "                # Find end of section (next section header or double newline)\n",
        "                end_patterns = [\n",
        "                    r'(?i)\\n\\s*(?:experience|education|projects?|certifications?|contact)',\n",
        "                    r'\\n\\s*[A-Z][A-Z\\s]+:',  # Next all-caps header\n",
        "                    r'\\n\\n\\n+',  # Multiple newlines\n",
        "                    r'\\n\\s*\\n\\s*[A-Z]'  # Double newline followed by capital letter\n",
        "                ]\n",
        "\n",
        "                end_pos = len(text)\n",
        "                for end_pattern in end_patterns:\n",
        "                    end_match = re.search(end_pattern, text[start_pos:])\n",
        "                    if end_match:\n",
        "                        potential_end = start_pos + end_match.start()\n",
        "                        if potential_end > start_pos + 20:  # Ensure reasonable section length\n",
        "                            end_pos = potential_end\n",
        "                            break\n",
        "\n",
        "                section_text = text[start_pos:end_pos].strip()\n",
        "                if len(section_text) > 5:\n",
        "                    skills_content.append(section_text)\n",
        "                    print(f\"   ğŸ“ Found section: {section_text[:50]}...\")\n",
        "\n",
        "        return ' '.join(skills_content) if skills_content else text\n",
        "\n",
        "    # Main extraction process\n",
        "    print(\"   ğŸ¯ Locating all skills sections...\")\n",
        "    skills_text = locate_skills_sections(text)\n",
        "\n",
        "    print(\"   ğŸ” Applying advanced skills matching...\")\n",
        "    print(f\"   ğŸ“ Processing text: {skills_text[:200]}...\")\n",
        "\n",
        "    found_skills = extract_skills_with_advanced_matching(skills_text)\n",
        "\n",
        "    # If few skills found, try entire document\n",
        "    if len(found_skills) < 3:\n",
        "        print(\"   ğŸ”„ Expanding search to full document...\")\n",
        "        found_skills.update(extract_skills_with_advanced_matching(text))\n",
        "\n",
        "    # Sort and return\n",
        "    final_skills = sorted(list(found_skills))\n",
        "\n",
        "    print(f\"   âœ… Extracted {len(final_skills)} skills: {final_skills}\")\n",
        "\n",
        "    return final_skills\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_name(text):\n",
        "    \"\"\"\n",
        "    Extract full name from resume text using multiple strategies\n",
        "    \"\"\"\n",
        "    print(\"ğŸ‘¤ Extracting name...\")\n",
        "    \n",
        "    # Strategy 1: Look for name patterns at the beginning\n",
        "    lines = text.split('\\n')\n",
        "    \n",
        "    # Common name indicators\n",
        "    name_indicators = [\n",
        "        r'^([A-Z][a-z]+ [A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',  # First line capitalized words\n",
        "        r'Name\\s*:?\\s*([A-Z][a-z]+ [A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)',\n",
        "        r'([A-Z][A-Z\\s]+)',  # All caps name\n",
        "    ]\n",
        "    \n",
        "    # Check first few lines for name patterns\n",
        "    for i, line in enumerate(lines[:5]):\n",
        "        line = line.strip()\n",
        "        if len(line) < 3 or len(line) > 50:  # Skip very short or long lines\n",
        "            continue\n",
        "            \n",
        "        # Skip lines with common resume keywords\n",
        "        skip_keywords = ['resume', 'cv', 'curriculum', 'vitae', 'phone', 'email', 'address', 'objective']\n",
        "        if any(keyword in line.lower() for keyword in skip_keywords):\n",
        "            continue\n",
        "            \n",
        "        for pattern in name_indicators:\n",
        "            match = re.search(pattern, line, re.IGNORECASE)\n",
        "            if match:\n",
        "                name = match.group(1).strip()\n",
        "                # Validate name (2-4 words, each starting with capital)\n",
        "                name_parts = name.split()\n",
        "                if 2 <= len(name_parts) <= 4 and all(part[0].isupper() for part in name_parts):\n",
        "                    print(f\"   âœ… Found name: {name}\")\n",
        "                    return name\n",
        "    \n",
        "    # Strategy 2: Use spaCy NER for person names\n",
        "    try:\n",
        "        doc = nlp(text[:500])  # Check first 500 chars\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"PERSON\" and len(ent.text.split()) >= 2:\n",
        "                name = ent.text.strip()\n",
        "                print(f\"   âœ… Found name via NER: {name}\")\n",
        "                return name\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"   âš ï¸ Name not found\")\n",
        "    return \"\"\n",
        "\n",
        "def extract_email(text):\n",
        "    \"\"\"\n",
        "    Extract email address from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“§ Extracting email...\")\n",
        "    \n",
        "    # Email regex pattern\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    \n",
        "    emails = re.findall(email_pattern, text)\n",
        "    \n",
        "    if emails:\n",
        "        # Return the first valid email\n",
        "        email = emails[0]\n",
        "        print(f\"   âœ… Found email: {email}\")\n",
        "        return email\n",
        "    \n",
        "    print(\"   âš ï¸ Email not found\")\n",
        "    return \"\"\n",
        "\n",
        "def extract_mobile(text):\n",
        "    \"\"\"\n",
        "    Extract mobile/phone number from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“± Extracting mobile number...\")\n",
        "    \n",
        "    # Phone number patterns (various formats)\n",
        "    phone_patterns = [\n",
        "        r'\\+\\d{1,3}[-.\\s]?\\d{10}',  # +91-9876543210\n",
        "        r'\\+\\d{1,3}\\s?\\(\\d{3}\\)\\s?\\d{3}[-.\\s]?\\d{4}',  # +1 (555) 123-4567\n",
        "        r'\\d{10}',  # 9876543210\n",
        "        r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # 987-654-3210\n",
        "        r'\\(\\d{3}\\)\\s?\\d{3}[-.\\s]?\\d{4}',  # (987) 654-3210\n",
        "    ]\n",
        "    \n",
        "    for pattern in phone_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            # Clean and format the phone number\n",
        "            phone = matches[0]\n",
        "            # Remove special characters except +\n",
        "            phone = re.sub(r'[^\\d+]', '', phone)\n",
        "            \n",
        "            # Validate length (should be 10-15 digits)\n",
        "            digits_only = re.sub(r'[^\\d]', '', phone)\n",
        "            if 10 <= len(digits_only) <= 15:\n",
        "                print(f\"   âœ… Found mobile: {phone}\")\n",
        "                return phone\n",
        "    \n",
        "    print(\"   âš ï¸ Mobile number not found\")\n",
        "    return \"\"\n",
        "\n",
        "def extract_linkedin(text):\n",
        "    \"\"\"\n",
        "    Extract LinkedIn profile URL\n",
        "    \"\"\"\n",
        "    print(\"ğŸ”— Extracting LinkedIn profile...\")\n",
        "    \n",
        "    linkedin_patterns = [\n",
        "        r'linkedin\\.com/in/[\\w\\-]+',\n",
        "        r'www\\.linkedin\\.com/in/[\\w\\-]+',\n",
        "        r'https?://(?:www\\.)?linkedin\\.com/in/[\\w\\-]+',\n",
        "    ]\n",
        "    \n",
        "    for pattern in linkedin_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            linkedin = matches[0]\n",
        "            if not linkedin.startswith('http'):\n",
        "                linkedin = 'https://' + linkedin\n",
        "            print(f\"   âœ… Found LinkedIn: {linkedin}\")\n",
        "            return linkedin\n",
        "    \n",
        "    print(\"   âš ï¸ LinkedIn not found\")\n",
        "    return \"\"\n",
        "\n",
        "def extract_github(text):\n",
        "    \"\"\"\n",
        "    Extract GitHub profile URL\n",
        "    \"\"\"\n",
        "    print(\"ğŸ™ Extracting GitHub profile...\")\n",
        "    \n",
        "    github_patterns = [\n",
        "        r'github\\.com/[\\w\\-]+',\n",
        "        r'www\\.github\\.com/[\\w\\-]+',\n",
        "        r'https?://(?:www\\.)?github\\.com/[\\w\\-]+',\n",
        "    ]\n",
        "    \n",
        "    for pattern in github_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            github = matches[0]\n",
        "            if not github.startswith('http'):\n",
        "                github = 'https://' + github\n",
        "            print(f\"   âœ… Found GitHub: {github}\")\n",
        "            return github\n",
        "    \n",
        "    print(\"   âš ï¸ GitHub not found\")\n",
        "    return \"\"\n",
        "\n",
        "def extract_portfolio(text):\n",
        "    \"\"\"\n",
        "    Extract portfolio/personal website URL\n",
        "    \"\"\"\n",
        "    print(\"ğŸŒ Extracting portfolio website...\")\n",
        "    \n",
        "    # Look for personal websites (excluding common platforms)\n",
        "    url_pattern = r'https?://(?:www\\.)?[\\w\\-]+\\.[\\w\\-]+(?:/[\\w\\-]*)*'\n",
        "    urls = re.findall(url_pattern, text, re.IGNORECASE)\n",
        "    \n",
        "    # Filter out common platforms\n",
        "    excluded_domains = ['linkedin.com', 'github.com', 'gmail.com', 'yahoo.com', 'outlook.com']\n",
        "    \n",
        "    for url in urls:\n",
        "        if not any(domain in url.lower() for domain in excluded_domains):\n",
        "            print(f\"   âœ… Found portfolio: {url}\")\n",
        "            return url\n",
        "    \n",
        "    print(\"   âš ï¸ Portfolio not found\")\n",
        "    return \"\"\n",
        "\n",
        "print(\"âœ… All extraction functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_education(text):\n",
        "    \"\"\"\n",
        "    Extract education information from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ Extracting education...\")\n",
        "    \n",
        "    education_list = []\n",
        "    \n",
        "    # Education section patterns\n",
        "    education_patterns = [\n",
        "        r'(?i)education\\s*:?',\n",
        "        r'(?i)academic\\s+background\\s*:?',\n",
        "        r'(?i)qualifications?\\s*:?',\n",
        "        r'(?i)degrees?\\s*:?'\n",
        "    ]\n",
        "    \n",
        "    # Find education section\n",
        "    education_section = \"\"\n",
        "    for pattern in education_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            start_pos = match.end()\n",
        "            # Find end of section\n",
        "            end_patterns = [\n",
        "                r'(?i)\\n\\s*(?:experience|work|employment|skills|projects?|certifications?)',\n",
        "                r'\\n\\s*[A-Z][A-Z\\s]+:',\n",
        "                r'\\n\\n\\n+'\n",
        "            ]\n",
        "            \n",
        "            end_pos = len(text)\n",
        "            for end_pattern in end_patterns:\n",
        "                end_match = re.search(end_pattern, text[start_pos:])\n",
        "                if end_match:\n",
        "                    end_pos = start_pos + end_match.start()\n",
        "                    break\n",
        "            \n",
        "            education_section = text[start_pos:end_pos].strip()\n",
        "            break\n",
        "    \n",
        "    if not education_section:\n",
        "        education_section = text  # Use full text if no section found\n",
        "    \n",
        "    # Degree patterns\n",
        "    degree_patterns = [\n",
        "        r'(B\\.?Tech|Bachelor of Technology|BTech)\\s+(?:in\\s+)?([^,\\n]+)',\n",
        "        r'(B\\.?E\\.?|Bachelor of Engineering)\\s+(?:in\\s+)?([^,\\n]+)',\n",
        "        r'(M\\.?Tech|Master of Technology|MTech)\\s+(?:in\\s+)?([^,\\n]+)',\n",
        "        r'(M\\.?S\\.?|Master of Science)\\s+(?:in\\s+)?([^,\\n]+)',\n",
        "        r'(MBA|Master of Business Administration)',\n",
        "        r'(Ph\\.?D\\.?|Doctor of Philosophy)\\s+(?:in\\s+)?([^,\\n]+)',\n",
        "        r'(B\\.?Sc\\.?|Bachelor of Science)\\s+(?:in\\s+)?([^,\\n]+)',\n",
        "        r'(M\\.?Sc\\.?|Master of Science)\\s+(?:in\\s+)?([^,\\n]+)',\n",
        "    ]\n",
        "    \n",
        "    # Extract degrees\n",
        "    for pattern in degree_patterns:\n",
        "        matches = re.findall(pattern, education_section, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            if isinstance(match, tuple):\n",
        "                degree = match[0]\n",
        "                field = match[1] if len(match) > 1 and match[1] else \"\"\n",
        "            else:\n",
        "                degree = match\n",
        "                field = \"\"\n",
        "            \n",
        "            # Look for institution and year nearby\n",
        "            degree_context = education_section\n",
        "            institution = \"\"\n",
        "            year = \"\"\n",
        "            grade = \"\"\n",
        "            \n",
        "            # Institution patterns\n",
        "            inst_patterns = [\n",
        "                r'(?:from\\s+|at\\s+)?([A-Z][^,\\n]+(?:University|Institute|College|School))',\n",
        "                r'([A-Z][^,\\n]+(?:University|Institute|College|School))',\n",
        "            ]\n",
        "            \n",
        "            for inst_pattern in inst_patterns:\n",
        "                inst_match = re.search(inst_pattern, degree_context, re.IGNORECASE)\n",
        "                if inst_match:\n",
        "                    institution = inst_match.group(1).strip()\n",
        "                    break\n",
        "            \n",
        "            # Year patterns\n",
        "            year_patterns = [\n",
        "                r'(20\\d{2})',\n",
        "                r'(19\\d{2})',\n",
        "                r'(\\d{4})'\n",
        "            ]\n",
        "            \n",
        "            for year_pattern in year_patterns:\n",
        "                year_match = re.search(year_pattern, degree_context)\n",
        "                if year_match:\n",
        "                    year = year_match.group(1)\n",
        "                    break\n",
        "            \n",
        "            # Grade patterns\n",
        "            grade_patterns = [\n",
        "                r'(\\d+\\.?\\d*\\s*(?:CGPA|GPA|cgpa|gpa))',\n",
        "                r'(\\d+\\.?\\d*%)',\n",
        "                r'(\\d+\\.?\\d*/10)',\n",
        "                r'(\\d+\\.?\\d*/4\\.0)'\n",
        "            ]\n",
        "            \n",
        "            for grade_pattern in grade_patterns:\n",
        "                grade_match = re.search(grade_pattern, degree_context)\n",
        "                if grade_match:\n",
        "                    grade = grade_match.group(1)\n",
        "                    break\n",
        "            \n",
        "            education_entry = {\n",
        "                \"degree\": f\"{degree} {field}\".strip(),\n",
        "                \"institution\": institution,\n",
        "                \"year\": year,\n",
        "                \"grade\": grade\n",
        "            }\n",
        "            \n",
        "            education_list.append(education_entry)\n",
        "    \n",
        "    print(f\"   âœ… Found {len(education_list)} education entries\")\n",
        "    return education_list\n",
        "\n",
        "def extract_experience(text):\n",
        "    \"\"\"\n",
        "    Extract work experience from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸ’¼ Extracting work experience...\")\n",
        "    \n",
        "    experience_list = []\n",
        "    \n",
        "    # Experience section patterns\n",
        "    exp_patterns = [\n",
        "        r'(?i)(?:work\\s+)?experience\\s*:?',\n",
        "        r'(?i)employment\\s+history\\s*:?',\n",
        "        r'(?i)professional\\s+experience\\s*:?',\n",
        "        r'(?i)career\\s+history\\s*:?'\n",
        "    ]\n",
        "    \n",
        "    # Find experience section\n",
        "    experience_section = \"\"\n",
        "    for pattern in exp_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            start_pos = match.end()\n",
        "            # Find end of section\n",
        "            end_patterns = [\n",
        "                r'(?i)\\n\\s*(?:education|skills|projects?|certifications?)',\n",
        "                r'\\n\\s*[A-Z][A-Z\\s]+:',\n",
        "                r'\\n\\n\\n+'\n",
        "            ]\n",
        "            \n",
        "            end_pos = len(text)\n",
        "            for end_pattern in end_patterns:\n",
        "                end_match = re.search(end_pattern, text[start_pos:])\n",
        "                if end_match:\n",
        "                    end_pos = start_pos + end_match.start()\n",
        "                    break\n",
        "            \n",
        "            experience_section = text[start_pos:end_pos].strip()\n",
        "            break\n",
        "    \n",
        "    if not experience_section:\n",
        "        # Look for company patterns in full text\n",
        "        experience_section = text\n",
        "    \n",
        "    # Company and role patterns\n",
        "    company_patterns = [\n",
        "        r'([A-Z][^,\\n]+(?:Inc|Ltd|LLC|Corp|Company|Technologies|Systems|Solutions|Pvt))',\n",
        "        r'([A-Z][a-zA-Z\\s&]+)\\s*(?:\\||,|\\n)',\n",
        "    ]\n",
        "    \n",
        "    role_patterns = [\n",
        "        r'(?:as\\s+)?([A-Z][^,\\n]+(?:Engineer|Developer|Analyst|Manager|Intern|Consultant))',\n",
        "        r'Position\\s*:\\s*([^,\\n]+)',\n",
        "        r'Role\\s*:\\s*([^,\\n]+)',\n",
        "    ]\n",
        "    \n",
        "    # Extract experience entries\n",
        "    lines = experience_section.split('\\n')\n",
        "    current_entry = {}\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) < 3:\n",
        "            continue\n",
        "        \n",
        "        # Check for company\n",
        "        for pattern in company_patterns:\n",
        "            match = re.search(pattern, line)\n",
        "            if match:\n",
        "                if current_entry:\n",
        "                    experience_list.append(current_entry)\n",
        "                current_entry = {\"company\": match.group(1).strip()}\n",
        "                break\n",
        "        \n",
        "        # Check for role\n",
        "        for pattern in role_patterns:\n",
        "            match = re.search(pattern, line)\n",
        "            if match:\n",
        "                current_entry[\"role\"] = match.group(1).strip()\n",
        "                break\n",
        "        \n",
        "        # Check for dates\n",
        "        date_patterns = [\n",
        "            r'((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4})\\s*[-â€“]\\s*((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{4}|Present)',\n",
        "            r'(\\d{4})\\s*[-â€“]\\s*(\\d{4}|Present)',\n",
        "            r'(\\d{1,2}/\\d{4})\\s*[-â€“]\\s*(\\d{1,2}/\\d{4}|Present)'\n",
        "        ]\n",
        "        \n",
        "        for pattern in date_patterns:\n",
        "            match = re.search(pattern, line)\n",
        "            if match:\n",
        "                current_entry[\"start_date\"] = match.group(1)\n",
        "                current_entry[\"end_date\"] = match.group(2)\n",
        "                break\n",
        "        \n",
        "        # Collect responsibilities\n",
        "        if \"responsibilities\" not in current_entry:\n",
        "            current_entry[\"responsibilities\"] = []\n",
        "        \n",
        "        # Look for bullet points or responsibilities\n",
        "        if re.match(r'^\\s*[â€¢Â·â–ªâ–«-]\\s*', line) or any(keyword in line.lower() for keyword in ['developed', 'built', 'created', 'managed', 'led', 'implemented']):\n",
        "            responsibility = re.sub(r'^\\s*[â€¢Â·â–ªâ–«-]\\s*', '', line).strip()\n",
        "            if responsibility:\n",
        "                current_entry[\"responsibilities\"].append(responsibility)\n",
        "    \n",
        "    if current_entry:\n",
        "        experience_list.append(current_entry)\n",
        "    \n",
        "    print(f\"   âœ… Found {len(experience_list)} experience entries\")\n",
        "    return experience_list\n",
        "\n",
        "def extract_projects(text):\n",
        "    \"\"\"\n",
        "    Extract projects from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ Extracting projects...\")\n",
        "    \n",
        "    projects_list = []\n",
        "    \n",
        "    # Projects section patterns\n",
        "    project_patterns = [\n",
        "        r'(?i)projects?\\s*:?',\n",
        "        r'(?i)personal\\s+projects?\\s*:?',\n",
        "        r'(?i)academic\\s+projects?\\s*:?',\n",
        "        r'(?i)key\\s+projects?\\s*:?'\n",
        "    ]\n",
        "    \n",
        "    # Find projects section\n",
        "    projects_section = \"\"\n",
        "    for pattern in project_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            start_pos = match.end()\n",
        "            # Find end of section\n",
        "            end_patterns = [\n",
        "                r'(?i)\\n\\s*(?:experience|education|skills|certifications?)',\n",
        "                r'\\n\\s*[A-Z][A-Z\\s]+:',\n",
        "                r'\\n\\n\\n+'\n",
        "            ]\n",
        "            \n",
        "            end_pos = len(text)\n",
        "            for end_pattern in end_patterns:\n",
        "                end_match = re.search(end_pattern, text[start_pos:])\n",
        "                if end_match:\n",
        "                    end_pos = start_pos + end_match.start()\n",
        "                    break\n",
        "            \n",
        "            projects_section = text[start_pos:end_pos].strip()\n",
        "            break\n",
        "    \n",
        "    if projects_section:\n",
        "        # Extract project entries\n",
        "        project_entries = re.split(r'\\n\\s*(?=[A-Z][^:]+:|\\d+\\.)', projects_section)\n",
        "        \n",
        "        for entry in project_entries:\n",
        "            if len(entry.strip()) < 10:\n",
        "                continue\n",
        "            \n",
        "            project = {}\n",
        "            lines = entry.split('\\n')\n",
        "            \n",
        "            # First line is usually project name\n",
        "            first_line = lines[0].strip()\n",
        "            project[\"name\"] = re.sub(r'^\\d+\\.\\s*', '', first_line)\n",
        "            \n",
        "            # Look for description and technologies\n",
        "            description_lines = []\n",
        "            technologies = []\n",
        "            \n",
        "            for line in lines[1:]:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                \n",
        "                # Check for technology indicators\n",
        "                if any(keyword in line.lower() for keyword in ['technologies', 'tech stack', 'built with', 'using']):\n",
        "                    # Extract technologies\n",
        "                    tech_text = re.sub(r'(?i)technologies?\\s*:?\\s*', '', line)\n",
        "                    tech_text = re.sub(r'(?i)tech\\s+stack\\s*:?\\s*', '', tech_text)\n",
        "                    tech_text = re.sub(r'(?i)built\\s+with\\s*:?\\s*', '', tech_text)\n",
        "                    tech_text = re.sub(r'(?i)using\\s*:?\\s*', '', tech_text)\n",
        "                    \n",
        "                    # Split by common separators\n",
        "                    techs = re.split(r'[,;|]', tech_text)\n",
        "                    technologies.extend([tech.strip() for tech in techs if tech.strip()])\n",
        "                else:\n",
        "                    description_lines.append(line)\n",
        "            \n",
        "            project[\"description\"] = ' '.join(description_lines)\n",
        "            project[\"technologies\"] = technologies\n",
        "            \n",
        "            # Look for links\n",
        "            link_pattern = r'https?://[\\w\\-\\.]+(?:/[\\w\\-]*)*'\n",
        "            links = re.findall(link_pattern, entry)\n",
        "            project[\"link\"] = links[0] if links else \"\"\n",
        "            \n",
        "            projects_list.append(project)\n",
        "    \n",
        "    print(f\"   âœ… Found {len(projects_list)} projects\")\n",
        "    return projects_list\n",
        "\n",
        "def extract_certifications(text):\n",
        "    \"\"\"\n",
        "    Extract certifications from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸ† Extracting certifications...\")\n",
        "    \n",
        "    certifications_list = []\n",
        "    \n",
        "    # Certifications section patterns\n",
        "    cert_patterns = [\n",
        "        r'(?i)certifications?\\s*:?',\n",
        "        r'(?i)certificates?\\s*:?',\n",
        "        r'(?i)licenses?\\s*:?',\n",
        "        r'(?i)achievements?\\s*:?'\n",
        "    ]\n",
        "    \n",
        "    # Find certifications section\n",
        "    cert_section = \"\"\n",
        "    for pattern in cert_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            start_pos = match.end()\n",
        "            # Find end of section\n",
        "            end_patterns = [\n",
        "                r'(?i)\\n\\s*(?:experience|education|skills|projects?)',\n",
        "                r'\\n\\s*[A-Z][A-Z\\s]+:',\n",
        "                r'\\n\\n\\n+'\n",
        "            ]\n",
        "            \n",
        "            end_pos = len(text)\n",
        "            for end_pattern in end_patterns:\n",
        "                end_match = re.search(end_pattern, text[start_pos:])\n",
        "                if end_match:\n",
        "                    end_pos = start_pos + end_match.start()\n",
        "                    break\n",
        "            \n",
        "            cert_section = text[start_pos:end_pos].strip()\n",
        "            break\n",
        "    \n",
        "    if cert_section:\n",
        "        # Extract certification entries\n",
        "        lines = cert_section.split('\\n')\n",
        "        \n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if len(line) < 5:\n",
        "                continue\n",
        "            \n",
        "            # Remove bullet points\n",
        "            line = re.sub(r'^\\s*[â€¢Â·â–ªâ–«-]\\s*', '', line)\n",
        "            \n",
        "            cert = {}\n",
        "            \n",
        "            # Look for certification name and issuer\n",
        "            # Pattern: \"Certification Name - Issuer (Year)\"\n",
        "            cert_pattern = r'([^-\\(]+)(?:\\s*-\\s*([^(]+))?(?:\\s*\\((\\d{4})\\))?'\n",
        "            match = re.search(cert_pattern, line)\n",
        "            \n",
        "            if match:\n",
        "                cert[\"name\"] = match.group(1).strip()\n",
        "                cert[\"issuer\"] = match.group(2).strip() if match.group(2) else \"\"\n",
        "                cert[\"year\"] = match.group(3) if match.group(3) else \"\"\n",
        "                \n",
        "                certifications_list.append(cert)\n",
        "    \n",
        "    print(f\"   âœ… Found {len(certifications_list)} certifications\")\n",
        "    return certifications_list\n",
        "\n",
        "def extract_summary(text):\n",
        "    \"\"\"\n",
        "    Extract summary/objective from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ Extracting summary/objective...\")\n",
        "    \n",
        "    # Summary section patterns\n",
        "    summary_patterns = [\n",
        "        r'(?i)(?:professional\\s+)?summary\\s*:?',\n",
        "        r'(?i)objective\\s*:?',\n",
        "        r'(?i)career\\s+objective\\s*:?',\n",
        "        r'(?i)profile\\s*:?',\n",
        "        r'(?i)about\\s+me\\s*:?'\n",
        "    ]\n",
        "    \n",
        "    for pattern in summary_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            start_pos = match.end()\n",
        "            # Find end of section\n",
        "            end_patterns = [\n",
        "                r'(?i)\\n\\s*(?:experience|education|skills|projects?)',\n",
        "                r'\\n\\s*[A-Z][A-Z\\s]+:',\n",
        "                r'\\n\\n'\n",
        "            ]\n",
        "            \n",
        "            end_pos = len(text)\n",
        "            for end_pattern in end_patterns:\n",
        "                end_match = re.search(end_pattern, text[start_pos:])\n",
        "                if end_match:\n",
        "                    end_pos = start_pos + end_match.start()\n",
        "                    break\n",
        "            \n",
        "            summary = text[start_pos:end_pos].strip()\n",
        "            if len(summary) > 20:  # Ensure it's substantial\n",
        "                print(f\"   âœ… Found summary: {summary[:50]}...\")\n",
        "                return summary\n",
        "    \n",
        "    print(\"   âš ï¸ Summary not found\")\n",
        "    return \"\"\n",
        "\n",
        "def extract_location(text):\n",
        "    \"\"\"\n",
        "    Extract location/address from resume text\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ Extracting location...\")\n",
        "    \n",
        "    # Location patterns\n",
        "    location_patterns = [\n",
        "        r'(?i)(?:address|location)\\s*:?\\s*([^,\\n]+(?:,\\s*[^,\\n]+)*)',\n",
        "        r'([A-Z][a-z]+,\\s*[A-Z][a-z]+,?\\s*[A-Z][a-z]+)',  # City, State, Country\n",
        "        r'([A-Z][a-z]+,\\s*[A-Z][a-z]+)',  # City, State\n",
        "    ]\n",
        "    \n",
        "    for pattern in location_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            location = matches[0]\n",
        "            if isinstance(location, tuple):\n",
        "                location = location[0]\n",
        "            print(f\"   âœ… Found location: {location}\")\n",
        "            return location.strip()\n",
        "    \n",
        "    print(\"   âš ï¸ Location not found\")\n",
        "    return \"\"\n",
        "\n",
        "print(\"âœ… Advanced extraction functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_resume_comprehensive(file_path):\n",
        "    \"\"\"\n",
        "    COMPREHENSIVE resume parsing pipeline that extracts ALL structured information.\n",
        "    Returns complete JSON with all fields as per the specification.\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"ğŸ¯ STARTING COMPREHENSIVE RESUME PARSING PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract raw text\n",
        "        raw_text = extract_text_from_resume(file_path)\n",
        "        if not raw_text:\n",
        "            return {\"error\": \"Failed to extract text from resume\"}\n",
        "\n",
        "        # Step 2: Clean and normalize text\n",
        "        cleaned_text = clean_and_normalize_text(raw_text)\n",
        "        if not cleaned_text:\n",
        "            return {\"error\": \"Text cleaning resulted in empty content\"}\n",
        "\n",
        "        # Step 3: Extract ALL entities\n",
        "        print(\"\\nğŸ” Extracting ALL structured information...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Basic Information\n",
        "        name = extract_name(cleaned_text)\n",
        "        email = extract_email(cleaned_text)\n",
        "        phone = extract_mobile(cleaned_text)\n",
        "        linkedin = extract_linkedin(cleaned_text)\n",
        "        github = extract_github(cleaned_text)\n",
        "        portfolio = extract_portfolio(cleaned_text)\n",
        "        location = extract_location(cleaned_text)\n",
        "        summary = extract_summary(cleaned_text)\n",
        "\n",
        "        # Complex Information\n",
        "        education = extract_education(cleaned_text)\n",
        "        skills = extract_skills(cleaned_text)\n",
        "        experience = extract_experience(cleaned_text)\n",
        "        projects = extract_projects(cleaned_text)\n",
        "        certifications = extract_certifications(cleaned_text)\n",
        "\n",
        "        # Step 4: Create comprehensive structured output\n",
        "        result = {\n",
        "            \"name\": name,\n",
        "            \"email\": email,\n",
        "            \"phone\": phone,\n",
        "            \"linkedin\": linkedin,\n",
        "            \"github\": github,\n",
        "            \"portfolio\": portfolio,\n",
        "            \"education\": education,\n",
        "            \"skills\": skills,\n",
        "            \"experience\": experience,\n",
        "            \"projects\": projects,\n",
        "            \"certifications\": certifications,\n",
        "            \"summary\": summary,\n",
        "            \"location\": location\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"âœ… COMPREHENSIVE PARSING COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"ğŸ“Š EXTRACTION SUMMARY:\")\n",
        "        print(f\"   ğŸ‘¤ Name: {name}\")\n",
        "        print(f\"   ğŸ“§ Email: {email}\")\n",
        "        print(f\"   ğŸ“± Phone: {phone}\")\n",
        "        print(f\"   ğŸ”— LinkedIn: {linkedin}\")\n",
        "        print(f\"   ğŸ™ GitHub: {github}\")\n",
        "        print(f\"   ğŸŒ Portfolio: {portfolio}\")\n",
        "        print(f\"   ğŸ“ Location: {location}\")\n",
        "        print(f\"   ğŸ“ Education: {len(education)} entries\")\n",
        "        print(f\"   ğŸ› ï¸ Skills: {len(skills)} skills\")\n",
        "        print(f\"   ğŸ’¼ Experience: {len(experience)} entries\")\n",
        "        print(f\"   ğŸš€ Projects: {len(projects)} entries\")\n",
        "        print(f\"   ğŸ† Certifications: {len(certifications)} entries\")\n",
        "        print(f\"   ğŸ“ Summary: {'Yes' if summary else 'No'}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ COMPREHENSIVE PARSING FAILED: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\"error\": f\"Parsing failed: {str(e)}\"}\n",
        "\n",
        "# Keep the old function for backward compatibility\n",
        "def parse_resume(file_path):\n",
        "    \"\"\"\n",
        "    Legacy function - calls the comprehensive parser\n",
        "    \"\"\"\n",
        "    return parse_resume_comprehensive(file_path)\n",
        "\n",
        "print(\"âœ… Comprehensive resume parser defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQQHQS1QjFhM",
        "outputId": "a305fd2a-1164-461c-8099-951cb199f626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Main parse_resume function defined!\n"
          ]
        }
      ],
      "source": [
        "def parse_resume(file_path):\n",
        "    \"\"\"\n",
        "    Main resume parsing pipeline that combines all extraction functions.\n",
        "    Returns structured JSON with full_name, email, mobile_number, and skills.\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ¯ STARTING RESUME PARSING PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract raw text\n",
        "        raw_text = extract_text_from_resume(file_path)\n",
        "        if not raw_text:\n",
        "            return {\"error\": \"Failed to extract text from resume\"}\n",
        "\n",
        "        # Step 2: Clean and normalize text\n",
        "        cleaned_text = clean_and_normalize_text(raw_text)\n",
        "        if not cleaned_text:\n",
        "            return {\"error\": \"Text cleaning resulted in empty content\"}\n",
        "\n",
        "        # Step 3: Extract entities\n",
        "        print(\"\\nğŸ” Extracting structured information...\")\n",
        "\n",
        "        full_name = extract_name(cleaned_text)\n",
        "        email = extract_email(cleaned_text)\n",
        "        mobile_number = extract_mobile(cleaned_text)\n",
        "        skills = extract_skills(cleaned_text)\n",
        "\n",
        "        # Step 4: Create structured output\n",
        "        result = {\n",
        "            \"full_name\": full_name,\n",
        "            \"email\": email,\n",
        "            \"mobile_number\": mobile_number,\n",
        "            \"skills\": skills\n",
        "        }\n",
        "\n",
        "        print(\"\\nâœ… PARSING COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"ğŸ“Š SUMMARY:\")\n",
        "        print(f\"   Name: {full_name}\")\n",
        "        print(f\"   Email: {email}\")\n",
        "        print(f\"   Mobile: {mobile_number}\")\n",
        "        print(f\"   Skills Count: {len(skills)}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ PARSING FAILED: {str(e)}\")\n",
        "        return {\"error\": f\"Parsing failed: {str(e)}\"}\n",
        "\n",
        "print(\"âœ… Main parse_resume function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "-v4ptcZDjJq7",
        "outputId": "e0a24531-ea90-407f-b03c-eabb90766c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¤ Upload your resume PDF file:\n",
            "========================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5fc41168-7652-4adb-96dd-23f24956b6e2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5fc41168-7652-4adb-96dd-23f24956b6e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Soham_2025.pdf to Soham_2025.pdf\n",
            "\n",
            "ğŸš€ Processing uploaded resume(s)...\n",
            "========================================\n",
            "\n",
            "ğŸ“‹ Processing: Soham_2025.pdf\n",
            "============================================================\n",
            "ğŸ¯ STARTING RESUME PARSING PIPELINE\n",
            "============================================================\n",
            "ğŸ“„ Processing: Soham_2025.pdf\n",
            "   ğŸ“Š Text length: 2474, Images: 0\n",
            "   ğŸ” PDF Type: DIGITAL\n",
            "âœ… Extraction complete: 2476 characters\n",
            "ğŸ§¹ Cleaning and normalizing text...\n",
            "   âœ… Cleaned text length: 2388 chars\n",
            "\n",
            "ğŸ” Extracting structured information...\n",
            "\n",
            "âŒ PARSING FAILED: name 'extract_name' is not defined\n",
            "\n",
            "ğŸ“„ PARSED RESUME DATA:\n",
            "========================================\n",
            "{\n",
            "  \"error\": \"Parsing failed: name 'extract_name' is not defined\"\n",
            "}\n",
            "========================================\n",
            "ğŸ’¾ Results saved to: Soham_2025_parsed.json\n",
            "\n",
            "âœ… Processing complete!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def format_resume_output(result):\n",
        "    \"\"\"\n",
        "    Format the resume parsing result for better display\n",
        "    \"\"\"\n",
        "    if \"error\" in result:\n",
        "        return result\n",
        "    \n",
        "    # Create formatted output matching the specification\n",
        "    formatted_result = {\n",
        "        \"name\": result.get(\"name\", \"\"),\n",
        "        \"email\": result.get(\"email\", \"\"),\n",
        "        \"phone\": result.get(\"phone\", \"\"),\n",
        "        \"linkedin\": result.get(\"linkedin\", \"\"),\n",
        "        \"github\": result.get(\"github\", \"\"),\n",
        "        \"portfolio\": result.get(\"portfolio\", \"\"),\n",
        "        \"education\": result.get(\"education\", []),\n",
        "        \"skills\": result.get(\"skills\", []),\n",
        "        \"experience\": result.get(\"experience\", []),\n",
        "        \"projects\": result.get(\"projects\", []),\n",
        "        \"certifications\": result.get(\"certifications\", []),\n",
        "        \"summary\": result.get(\"summary\", \"\"),\n",
        "        \"location\": result.get(\"location\", \"\")\n",
        "    }\n",
        "    \n",
        "    return formatted_result\n",
        "\n",
        "print(\"ğŸ¯ COMPREHENSIVE RESUME PARSER\")\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ“¤ Upload your resume PDF file to extract ALL information:\")\n",
        "print(\"   â€¢ Personal Information (Name, Email, Phone, LinkedIn, GitHub)\")\n",
        "print(\"   â€¢ Education (Degree, Institution, Year, Grade)\")\n",
        "print(\"   â€¢ Work Experience (Company, Role, Dates, Responsibilities)\")\n",
        "print(\"   â€¢ Projects (Name, Description, Technologies, Links)\")\n",
        "print(\"   â€¢ Skills (Technical and Soft Skills)\")\n",
        "print(\"   â€¢ Certifications (Name, Issuer, Year)\")\n",
        "print(\"   â€¢ Summary/Objective\")\n",
        "print(\"   â€¢ Location/Address\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    print(\"\\nğŸš€ Processing uploaded resume(s) with COMPREHENSIVE extraction...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for file_name in uploaded.keys():\n",
        "        print(f\"\\nğŸ“‹ Processing: {file_name}\")\n",
        "        \n",
        "        # Parse resume with comprehensive extraction\n",
        "        result = parse_resume_comprehensive(file_name)\n",
        "        \n",
        "        # Format the result\n",
        "        formatted_result = format_resume_output(result)\n",
        "        \n",
        "        print(f\"\\nğŸ“„ COMPREHENSIVE RESUME DATA (JSON FORMAT):\")\n",
        "        print(\"=\"*60)\n",
        "        print(json.dumps(formatted_result, indent=2, ensure_ascii=False))\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Save results to JSON file with timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_filename = f\"{file_name.replace('.pdf', '')}_parsed_{timestamp}.json\"\n",
        "        \n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(formatted_result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"ğŸ’¾ Results saved to: {output_filename}\")\n",
        "        \n",
        "        # Display summary statistics\n",
        "        if \"error\" not in formatted_result:\n",
        "            print(f\"\\nğŸ“Š EXTRACTION STATISTICS:\")\n",
        "            print(f\"   âœ… Personal Info: {sum(1 for field in ['name', 'email', 'phone'] if formatted_result.get(field))}/3\")\n",
        "            print(f\"   âœ… Social Links: {sum(1 for field in ['linkedin', 'github', 'portfolio'] if formatted_result.get(field))}/3\")\n",
        "            print(f\"   âœ… Education Entries: {len(formatted_result.get('education', []))}\")\n",
        "            print(f\"   âœ… Skills Found: {len(formatted_result.get('skills', []))}\")\n",
        "            print(f\"   âœ… Experience Entries: {len(formatted_result.get('experience', []))}\")\n",
        "            print(f\"   âœ… Projects: {len(formatted_result.get('projects', []))}\")\n",
        "            print(f\"   âœ… Certifications: {len(formatted_result.get('certifications', []))}\")\n",
        "            print(f\"   âœ… Summary: {'Yes' if formatted_result.get('summary') else 'No'}\")\n",
        "            print(f\"   âœ… Location: {'Yes' if formatted_result.get('location') else 'No'}\")\n",
        "        \n",
        "        # Optionally download the JSON file\n",
        "        print(f\"\\nğŸ’¾ To download the JSON file, uncomment the line below:\")\n",
        "        print(f\"# files.download('{output_filename}')\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No files uploaded!\")\n",
        "\n",
        "print(\"\\nâœ… COMPREHENSIVE RESUME PARSING COMPLETE!\")\n",
        "print(\"ğŸ¯ The parser now extracts ALL resume information as per specification!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
